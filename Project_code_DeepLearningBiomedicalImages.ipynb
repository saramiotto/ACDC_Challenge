{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing library**"
      ],
      "metadata": {
        "id": "_C7TjaBC4jFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSfi7Mw3VVzo"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# Install specific versions of PyTorch and torchvision compiled with the same CUDA version (11.7)\n",
        "# Note: You should check the torchvision version compatible with PyTorch 2.0.0, and replace 'compatible_version' with the actual version number.\n",
        "!pip install torch==2.0.0+cu117 torchvision==compatible_version+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchviz SimpleITK matplotlib monai 'monai[all]'\n",
        "!pip install --upgrade monai\n",
        "!pip install nibabel\n",
        "\n",
        "# Import libraries\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from os import makedirs\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from __future__ import print_function, division\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "import torch.utils.data as data\n",
        "import monai\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, AddChanneld, RandFlipd, RandRotated, ToTensorD, CenterSpatialCropd,\n",
        "    ScaleIntensityd, AsDiscreted\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmpBfvp-TuuB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Automated Cardiac Diagnosis Challenge**"
      ],
      "metadata": {
        "id": "sYMUMLuq1sir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cardiac Magnetic Resonance Imaging is the most effective method used for detecting and treating cardiac disorders which are the world’s first cause of death. Manual analysis of cardiac images is time-consuming and prone to errors, necessitating a precise and automated segmentation approach. Convolutional Neural Networks (CNNs), particularly Fully Convolutional Networks (FCN) and U-Net, have emerged as successful methods for medical image segmentation. U-Net, a 2D CNN architecture, shows respectable accuracy by incorporating skip links. This work aims to solve the segmentation and classification task proposed by the ACDC challenge, utilizing the ACDC dataset, which includes expert manual segmentations and various clinical diagnoses."
      ],
      "metadata": {
        "id": "KrLnJ3xq2EiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset preparation**"
      ],
      "metadata": {
        "id": "tso1VGl74ou1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell performs a series of commands to facilitate the handling of a dataset. Firstly, it creates a new directory named \"dataset\" where the dataset will be stored. Then, it copies a file called \"Resources.zip\" from a Google Drive folder and pastes it into the \"dataset\" directory. Finally, it unzips the \"Adaptiope.zip\" file within the \"dataset\" directory, extracting its contents for further analysis and exploration. These commands enable the setup and preparation of the dataset for subsequent data manipulation tasks."
      ],
      "metadata": {
        "id": "phUrDiXV1Svs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4z5yymPUAJ3"
      },
      "outputs": [],
      "source": [
        "# Create a new directory called \"dataset\"\n",
        "!mkdir dataset\n",
        "\n",
        "# Copy the file \"Resources.zip\" from the Google Drive folder to the \"dataset\" directory\n",
        "!cp \"/content/drive/MyDrive/Resources\" dataset/\n",
        "\n",
        "# Unzip the \"Adaptiope.zip\" file in the \"dataset\" directory\n",
        "!unzip /content/drive/MyDrive/Resources.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Segmentation**"
      ],
      "metadata": {
        "id": "feByXOwt4wlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Automated Cardiac Diagnosis Challenge is a challenge that aims to compare the performance of automatic methods on the segmentation of the left ventricular endocardium and epicardium as well as the right ventricular endocardium for both end diastolic and end systolic phase instances. Segmentation of cardiac anatomical structures in cardiac magnetic resonance images (CMRI) is a prerequisite for automatic diagnosis and prognosis of cardiovascular diseases.\n",
        "\n",
        "One study combined automatic segmentation and assessment of segmentation uncertainty in CMRI to detect image regions containing local segmentation failures. The experiments revealed that combining automatic segmentation with manual correction of detected segmentation failures resulted in improved segmentation and a 10-fold reduction of expert time compared to manual expert segmentation."
      ],
      "metadata": {
        "id": "uM0YbFyt1coc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-processing**"
      ],
      "metadata": {
        "id": "6lpnfTv_woLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing for the ACDC challenge involved normalizing image intensities, cropping to 128x128 pixels, applying random flips and rotating within π/4 radians, achieved using MONAI library functions. The preprocessing pipeline ensured standardized data, augmented dataset variability, and facilitated subsequent analysis and classification."
      ],
      "metadata": {
        "id": "FZpP2DEi2gXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell contains a function called read_data2 that reads data from a specified path, mode, and label. It retrieves paths of image files from the provided directory structure, organizes them, and returns the list of image file paths. The function streamlines the data retrieval process, making it easier to handle and analyze the dataset."
      ],
      "metadata": {
        "id": "9qJXZGwf3AGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data2(path, mode=\"training\", label=\"images\"):\n",
        "    # Function to read data from a specified path, mode, and label\n",
        "\n",
        "    path_img = []  # List to store paths of image files\n",
        "    paths = glob.glob(path + mode + \"/\" + label + \"/*\")  # Get list of paths in the specified mode and label\n",
        "    paths.sort()  # Sort the paths in alphabetical order\n",
        "\n",
        "    for element in paths:\n",
        "        #path_slice = glob.glob(element+\"/\")\n",
        "        path_slice = element + \"/\"  # Get the path of the slice\n",
        "\n",
        "        file_list = glob.glob(glob.escape(path_slice[0]) + \"*.gz\")  # Get the list of files in the slice\n",
        "        sorted_file_list = sorted(file_list)  # Sort the files in the slice\n",
        "\n",
        "        path_img.append(element)  # Add the image file path to the list\n",
        "\n",
        "    return path_img  # Return the list of image file paths"
      ],
      "metadata": {
        "id": "ZiJ6tK3TCWQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell dictionaries which create a corrispondence between images and masks are created for the training, validation, and test set.\n",
        "\n",
        "Note that for the training three paths of images and masks have been given, each of them correspond to the transformed images obtained for the purpose of data augmentation. This has been done with the code snipped \"Data Augmentation\" (below the section \"Segmentation\"). Of course this does not involved the independent test data in order of not having biased results."
      ],
      "metadata": {
        "id": "zxF9z1F93E-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/database/\"  # Path to the database\n",
        "\n",
        "# Read image and mask paths for training data with different labels\n",
        "path_img = read_data2(path, mode=\"training\", label=\"images\")\n",
        "path_mask = read_data2(path, mode=\"training\", label=\"masks\")\n",
        "\n",
        "# Split the training data into train and validation sets\n",
        "train_size = int(len(path_img) * 0.8)\n",
        "train_img = path_img[:train_size]\n",
        "train_mask = path_mask[:train_size]\n",
        "val_img = path_img[train_size:]\n",
        "val_mask = path_mask[train_size:]\n",
        "\n",
        "# Read image and mask paths for augmented training data with different labels\n",
        "path_img2 = read_data2(path, mode=\"training\", label=\"images2\")\n",
        "path_mask2 = read_data2(path, mode=\"training\", label=\"masks2\")\n",
        "\n",
        "path_img3 = read_data2(path, mode=\"training\", label=\"images3\")\n",
        "path_mask3 = read_data2(path, mode=\"training\", label=\"masks3\")\n",
        "\n",
        "# Combine original and augmented image and mask paths for the training set\n",
        "train_img = train_img + path_img2 + path_img3\n",
        "train_mask = train_mask + path_mask2 + path_mask3\n",
        "\n",
        "# Create a DataFrame for training and validation data\n",
        "train_data = pd.DataFrame({'img': train_img, 'mask': train_mask})\n",
        "val_data = pd.DataFrame({'img': val_img, 'mask': val_mask})\n",
        "\n",
        "# Convert the DataFrames to lists of dictionaries\n",
        "train_dicts = train_data.to_dict('records')\n",
        "val_dicts = val_data.to_dict('records')\n",
        "\n",
        "# Read image and mask paths for testing data\n",
        "path_img_test = read_data2(path, mode=\"testing2\", label=\"images\")\n",
        "path_mask_test = read_data2(path, mode=\"testing2\", label=\"masks\")\n",
        "test_dicts = pd.DataFrame({'img': path_img_test, 'mask': path_mask_test})  # Create a DataFrame for testing data\n",
        "\n",
        "test_dicts = test_dicts.to_dict('records')  # Convert the DataFrame to a list of dictionaries"
      ],
      "metadata": {
        "id": "kfMdS0tfCYoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, we encounter a class called LoadAndTransform that encapsulates a data loading and transformation pipeline. This elegant pipeline facilitates the preparation and augmentation of data for further analysis. Let's explore its intricacies:\n",
        "\n",
        "Within this method, a series of transformations are defined using the Compose function from the MONAI library. Each transformation plays a unique role in preparing the data for subsequent tasks.\n",
        "\n",
        "The transformation pipeline begins by loading the image and mask data using the LoadImaged transformation. It gracefully handles the retrieval of these essential components, setting the stage for subsequent operations.\n",
        "\n",
        "The next transformation, AddChanneld, augments the image and mask data by adding a channel dimension. This dimensionality enhancement ensures compatibility with subsequent processing steps.\n",
        "\n",
        "To normalize the intensities of the image, the ScaleIntensityd transformation gracefully standardizes the data, fostering consistency and facilitating subsequent analysis.\n",
        "\n",
        "The pipeline continues with the CenterSpatialCropd transformation, which expertly performs a centered spatial crop on both the image and mask. This operation helps focus on the essential regions of interest, promoting accurate and efficient analysis.\n",
        "\n",
        "RandFlipd ia a transformation that randomly flips the image and mask. This augmentation technique enhances the diversity of the dataset, promoting robustness and reducing bias.\n",
        "\n",
        "Next, the RandRotated transformation takes the stage, elegantly introducing random rotations to the image and mask. By varying the orientation, the dataset becomes more comprehensive and resilient to orientation-based discrepancies.\n",
        "\n",
        "To enable one-hot encoding, the AsDiscreted transformation gracefully converts the mask into a one-hot representation. This representation facilitates multi-class classification tasks by encoding each class as a separate channel.\n",
        "\n",
        "ToTensorD transformation converts the image and mask into PyTorch tensors. This conversion facilitates seamless integration with PyTorch-based modeling frameworks."
      ],
      "metadata": {
        "id": "dqBdvZtL32E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4  # Number of classes for one-hot encoding\n",
        "\n",
        "class LoadAndTransform_train:\n",
        "    def __init__(self):\n",
        "        # Initialize the data loading and transformation pipeline\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['img', 'mask']),  # Load the image and mask data\n",
        "            AddChanneld(keys=['img', 'mask']),  # Add channel dimension to the image and mask\n",
        "            ScaleIntensityd(keys=['img']),  # Normalize intensities of the image\n",
        "            CenterSpatialCropd(keys=['img', 'mask'], roi_size=[128, 128]),  # Center crop the image and mask\n",
        "            RandFlipd(keys=['img', 'mask'], prob=0.5, spatial_axis=1),  # Randomly flip the image and mask\n",
        "            RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest']),  # Randomly rotate the image and mask\n",
        "            AsDiscreted(keys=['mask'], to_onehot=num_classes, num_classes=num_classes),  # Convert the mask to one-hot encoding\n",
        "            ToTensorD(keys=['img', 'mask'])  # Convert the image and mask to PyTorch tensors\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # Apply the transformation pipeline on the given sample\n",
        "        return self.transform(sample)"
      ],
      "metadata": {
        "id": "UbjLJGITCcKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class dataset for the training set is created."
      ],
      "metadata": {
        "id": "vnBMAXYU5KnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = LoadAndTransform_train()  # Instantiate the training data transformation pipeline\n",
        "train_dataset = monai.data.CacheDataset(train_dicts, train_transform)  # Create a training dataset with the transformed data"
      ],
      "metadata": {
        "id": "hrjiGHR9CjMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell executes the same operations of the one above, but not rotations, or flipping as we are dealing with the test and the validation."
      ],
      "metadata": {
        "id": "i1cLow2z41Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4  # Number of classes for one-hot encoding\n",
        "\n",
        "class LoadAndTransform:\n",
        "    def __init__(self):\n",
        "        # Initialize the data loading and transformation pipeline\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['img', 'mask']),  # Load the image and mask data\n",
        "            AddChanneld(keys=['img', 'mask']),  # Add channel dimension to the image and mask\n",
        "            ScaleIntensityd(keys=['img']),  # Normalize intensities of the image\n",
        "            CenterSpatialCropd(keys=['img', 'mask'], roi_size=[128, 128]),  # Center crop the image and mask\n",
        "            AsDiscreted(keys=['mask'], to_onehot=num_classes, num_classes=num_classes),  # Convert the mask to one-hot encoding\n",
        "            ToTensorD(keys=['img', 'mask'])  # Convert the image and mask to PyTorch tensors\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # Apply the transformation pipeline on the given sample\n",
        "        return self.transform(sample)"
      ],
      "metadata": {
        "id": "tmOY-h1R4sFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the class dataset for validation and test is implemented."
      ],
      "metadata": {
        "id": "pStwxl8X5DDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_transform = LoadAndTransform()  # Instantiate the validation data transformation pipeline\n",
        "validation_dataset = monai.data.CacheDataset(val_dicts, validation_transform)  # Create a validation dataset with the transformed data\n",
        "\n",
        "test_transform = LoadAndTransform()  # Instantiate the testing data transformation pipeline\n",
        "test_dataset = monai.data.CacheDataset(test_dicts, test_transform)  # Create a testing dataset with the transformed data"
      ],
      "metadata": {
        "id": "FHsg6DjT4ppP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloaders instantiation."
      ],
      "metadata": {
        "id": "3FgAIKGg5Rbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataloader\n",
        "dataloader_train = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "dataloader_validation = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "dataloader_test = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "W13KXjLTCmDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Augmentation**"
      ],
      "metadata": {
        "id": "0ppOiB9btjFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the field of machine learning, one of the critical factors that significantly impacts model performance is the availability and quality of the training data. However, it is not uncommon to encounter scenarios where the training dataset is relatively small, limiting the model's ability to learn complex patterns and generalize well. To mitigate this challenge, researchers and practitioners often employ data augmentation techniques to artificially expand the training set, thereby enhancing its dimensionality. This essay explores the process of data augmentation, specifically focusing on creating additional images through rotation and flipping, and the organizational steps involved in preparing the augmented dataset in Google Drive.\n",
        "\n",
        "Data Augmentation for Image Classification:\n",
        "In image classification tasks, data augmentation serves as a powerful tool to increase the diversity of the training set by applying various transformations to the existing images. By introducing such transformations, the model becomes exposed to a wider range of scenarios, leading to improved generalization and robustness. Among the commonly used data augmentation techniques, rotation and flipping have proven to be effective in expanding the dataset.\n",
        "\n",
        "Rotation:\n",
        "The process of rotation involves rotating an image by a certain degree within a specified range. For instance, an original image can be rotated by 90, 180, or 270 degrees to create three additional variations. By randomly selecting rotation angles, we can generate multiple images with different orientations, increasing the training set's dimensionality. This augmented dataset will enable the model to learn and classify objects irrespective of their orientation, making it more adaptable to real-world scenarios.\n",
        "\n",
        "Flipping:\n",
        "Flipping is another widely employed data augmentation technique that horizontally or vertically mirrors an image. This transformation creates new instances by reversing the original image along the specified axis. By applying horizontal flips, we can generate images that exhibit a mirrored effect, contributing to increased variation in the training set. Similarly, vertical flips provide additional diversity by introducing an upside-down perspective. The augmented dataset resulting from these flips will help the model to be more resilient to changes in object orientation and achieve better performance.\n",
        "\n",
        "Organizational Steps in Google Drive:\n",
        "Before implementing the data augmentation process, it is essential to organize the augmented dataset in a structured manner. Google Drive provides a convenient platform for creating and storing the augmented images in separate folders. By creating distinct folders for rotated and flipped images, we can maintain a clear distinction between the original and augmented dataset.\n",
        "\n",
        "To begin, a dedicated folder can be created in Google Drive to house the training dataset. Within this main folder, separate subfolders should be created for rotated and flipped images. These subfolders serve as the destination for the augmented images generated through the data augmentation process.\n",
        "\n",
        "Namely, inside training folders \"images\", \"masks\", \"images2\", \"masks2\", \"images3\", \"masks3\" have to be created in advance to allow the code to run.\n",
        "\n",
        "By structuring the dataset in this manner, it becomes easier to keep track of the original and augmented images during the training process. Furthermore, it facilitates the integration of the augmented dataset with popular machine learning frameworks, as they often require the dataset to be organized into training, validation, and testing subsets."
      ],
      "metadata": {
        "id": "xvBv46CbuV8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path, mode=\"training\"):\n",
        "\n",
        "    path_gt = []  # Initialize an empty list to store the paths of ground truth files\n",
        "    path_img = []  # Initialize an empty list to store the paths of image files\n",
        "\n",
        "    paths = glob.glob(data_path + mode + \"/*\")  # Get all the paths of files in the specified mode (e.g., \"training\")\n",
        "    paths.sort()  # Sort the paths in alphabetical order\n",
        "\n",
        "    for element in paths:  # Iterate through each path in the list of paths\n",
        "        if (element[-10:-3]) == \"patient\":  # Check if the path corresponds to a patient\n",
        "            path_slice = glob.glob(element + \"/\")  # Get the path of the slice directory\n",
        "\n",
        "            file_list = glob.glob(glob.escape(path_slice[0]) + \"*.gz\")  # Get all the file paths within the slice directory\n",
        "            sorted_file_list = sorted(file_list)  # Sort the file paths in alphabetical order\n",
        "\n",
        "            for element in sorted_file_list:  # Iterate through each sorted file path\n",
        "                if (element[-9:-7]) != \"4d\":  # Check if the file path does not contain \"4d\"\n",
        "                    if (element[-9:-7]) == \"gt\":  # Check if the file path contains \"gt\"\n",
        "                        path_gt.append(element)  # Append the ground truth file path to the list\n",
        "                    else:\n",
        "                        path_img.append(element)  # Append the image file path to the list\n",
        "\n",
        "    dicts = pd.DataFrame({'img': path_img, 'mask': path_gt})  # Create a DataFrame with image and mask paths\n",
        "    return dicts"
      ],
      "metadata": {
        "id": "TdXOmk1G2YG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/database/'\n",
        "\n",
        "# Create a dictionary between the paths of the images and the paths of the masks within the train\n",
        "train_dicts = read_data(data_path, mode='training')\n",
        "train_dicts = train_dicts.to_dict('records')"
      ],
      "metadata": {
        "id": "SGHTV7QR2eKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, AddChanneld, RandFlipd, RandRotated, ToTensorD, CenterSpatialCropd, DivisiblePadD, SqueezeDimd, AsDiscreted\n",
        ")\n",
        "\n",
        "num_classes = 4  # Number of classes in the dataset\n",
        "\n",
        "class LoadAndTransform:\n",
        "    def __init__(self):\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['img', 'mask']),  # Load the image and mask using specified keys\n",
        "            AddChanneld(keys=['img', 'mask']),  # Add a channel dimension to the image and mask\n",
        "            CenterSpatialCropd(keys=['img', 'mask'], roi_size=[128, 128, 1]),  # Crop the image and mask to a specific size\n",
        "            SqueezeDimd(keys=['img', 'mask'], dim=-1),  # Remove the last dimension from the image and mask\n",
        "            RandFlipd(keys=['img', 'mask'], prob=0.5, spatial_axis=1),  # Randomly flip the image and mask along the spatial axis\n",
        "            RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest']),  # Randomly rotate the image and mask within a specified range\n",
        "            AsDiscreted(keys=['mask'], to_onehot=num_classes, num_classes=num_classes),  # Convert the mask to one-hot encoding\n",
        "            ToTensorD(keys=['img', 'mask'])  # Convert the image and mask to PyTorch tensors\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self.transform(sample)  # Apply the defined transformation to the input sample"
      ],
      "metadata": {
        "id": "WZfZOcVu4wZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following snippet is used to rotate both the original training image and mask of 45 degrees."
      ],
      "metadata": {
        "id": "gTh-3jTdyWnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_rotation(dataset):\n",
        "    masks = []  # Initialize an empty list to store the rotated masks\n",
        "    images = []  # Initialize an empty list to store the rotated images\n",
        "    metadata = []  # Initialize an empty list to store the metadata\n",
        "    num = None  # Variable to check whether the index is repeated or not per iteration\n",
        "    num2 = []  # Initialize an empty list\n",
        "\n",
        "    # Load sample\n",
        "    for element in range(len(dataset)):\n",
        "        sample_dict = dataset[element]  # Get the sample from the dataset\n",
        "\n",
        "        # Add channels\n",
        "        add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask'])  # Initialize the transform\n",
        "        channels_sample_dict = add_channels_transform(sample_dict)  # Apply the transform\n",
        "        print(\"Size of the image before AddChanneld transform\", sample_dict[\"img\"].shape)\n",
        "        print(\"Size of the image after AddChanneld transform\", channels_sample_dict[\"img\"].shape)\n",
        "\n",
        "        random_rotation_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
        "        rotated_sample_dict = random_rotation_transform(channels_sample_dict)  # Apply the random rotation transform\n",
        "        masks.append(rotated_sample_dict[\"mask\"])  # Append the rotated mask to the masks list\n",
        "        images.append(rotated_sample_dict[\"img\"])  # Append the rotated image to the images list\n",
        "        metadata.append(rotated_sample_dict[\"img_meta_dict\"][\"filename_or_obj\"])  # Append the filename or object metadata\n",
        "    return metadata, masks, images  # Return the metadata, masks, and images lists"
      ],
      "metadata": {
        "id": "1OIaAy1E0Ic2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general_data,masks,images=data_rotation(train_dataset)"
      ],
      "metadata": {
        "id": "2aCC8FUP4RYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this cell is to save the generated images and masks in the NIfTI format (.nii.gz)."
      ],
      "metadata": {
        "id": "8IiphDSgy6kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "def save_nifti(in_image, in_label, out, index, general_data):\n",
        "    # Convert the torch tensors into numpy array\n",
        "    volume = np.array(in_image.detach().cpu()[0, :, :, :], dtype=np.float32)\n",
        "    lab = np.array(in_label.detach().cpu()[0, :, :, :], dtype=np.float32)\n",
        "\n",
        "    # Convert the numpy array into nifti file\n",
        "    volume = nib.Nifti1Image(volume, np.eye(4))\n",
        "    lab = nib.Nifti1Image(lab, np.eye(4))\n",
        "\n",
        "    # Create the path to save the images and labels\n",
        "    path_out_images = os.path.join(out, 'Images')\n",
        "    path_out_labels = os.path.join(out, 'Labels')\n",
        "\n",
        "    # Make directory if not existing\n",
        "    if not os.path.exists(path_out_images):\n",
        "        os.mkdir(path_out_images)\n",
        "    if not os.path.exists(path_out_labels):\n",
        "        os.mkdir(path_out_labels)\n",
        "\n",
        "    path_data = os.path.join(out, 'Images')\n",
        "    path_label = os.path.join(out, 'Labels')\n",
        "    nib.save(volume, os.path.join(path_data, f'patient_rotation'+general_data[index][53:-7]+'.nii.gz'))\n",
        "    nib.save(lab, os.path.join(path_label, f'patient_rotation_'+general_data[index][53:-7]+'.nii.gz'))\n",
        "\n",
        "    print(f'patient_generated_{index} is saved', end='\\r')"
      ],
      "metadata": {
        "id": "sj-ZH8rP19KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = '/content/drive/MyDrive/rotation/'"
      ],
      "metadata": {
        "id": "QGJzTPFO5lQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This again create new images and masks by rotating the previous ones by other 45 degrees."
      ],
      "metadata": {
        "id": "jXZzExrYy_Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_rotation2(dataset):\n",
        "    masks=[]\n",
        "    images=[]\n",
        "    metadata=[]\n",
        "    num=None # check whether the index is repeated or not per iteration\n",
        "    num2=[]\n",
        "    # Load sample\n",
        "    for element in range(len(dataset)):\n",
        "\n",
        "      sample_dict = dataset[element]\n",
        "\n",
        "      # Add channels\n",
        "      add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask']) # Initialize the transform\n",
        "      channels_sample_dict = add_channels_transform(sample_dict) # Apply the transform\n",
        "      print(\"Size of the image before AddChanneld transform\", sample_dict[\"img\"].shape)\n",
        "      print(\"Size of the image after AddChanneld transform\", channels_sample_dict[\"img\"].shape)\n",
        "\n",
        "      random_rotation_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=3*np.pi/4, prob=1, mode=['bilinear', 'nearest']) # rotation of 135 degrees\n",
        "      rotated_sample_dict = random_rotation_transform(channels_sample_dict)\n",
        "      masks.append(rotated_sample_dict[\"mask\"])\n",
        "      images.append(rotated_sample_dict[\"img\"])\n",
        "      metadata.append(rotated_sample_dict[\"img_meta_dict\"][\"filename_or_obj\"])\n",
        "    return metadata,masks,images"
      ],
      "metadata": {
        "id": "vIrDFjulcyyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving these new elemnts in ou2."
      ],
      "metadata": {
        "id": "sFHWIAi8zIJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = '/content/drive/MyDrive/rotation2/'\n",
        "for i,element in enumerate(images):\n",
        "  save_nifti(element, masks[i], out2, i,general_data)"
      ],
      "metadata": {
        "id": "UK3N9ELqdIrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this following code is to read the file paths of augmented image and mask files stored in a specified directory. It assumes that the augmented images are stored in the \"Images\" subdirectory and the corresponding augmented masks are stored in the \"Labels\" subdirectory within the given path."
      ],
      "metadata": {
        "id": "cppf1WUF0GY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_augmented(path):\n",
        "    path_gt = []  # Initialize an empty list to store the paths of ground truth files\n",
        "    path_img = []  # Initialize an empty list to store the paths of image files\n",
        "\n",
        "    path_images = path + \"/Images\"  # Path to the images directory\n",
        "    path_masks = path + \"/Labels\"  # Path to the labels directory\n",
        "\n",
        "    paths1 = glob.glob(path_images + \"/*\")  # Get all the paths of files in the images directory\n",
        "    paths2 = glob.glob(path_masks + \"/*\")  # Get all the paths of files in the labels directory\n",
        "\n",
        "    paths1.sort()  # Sort the paths of images in alphabetical order\n",
        "    paths2.sort()  # Sort the paths of labels in alphabetical order\n",
        "\n",
        "    for i, element in enumerate(paths1):  # Iterate through each path in the list of image paths\n",
        "        path_img.append(element)  # Append the image file path to the list\n",
        "        path_gt.append(paths2[i])  # Append the corresponding ground truth file path to the list\n",
        "\n",
        "    dicts = pd.DataFrame({'img': path_img, 'mask': path_gt})  # Create a DataFrame with image and mask paths\n",
        "    return dicts"
      ],
      "metadata": {
        "id": "iExN3JZ56C5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_dataset = read_augmented(out)\n",
        "rotated_dataset = rotated_dataset.to_dict('records')\n",
        "dataset=train_dicts+rotated_dataset\n",
        "rotation_transf = LoadAndTransform()\n",
        "dataset = monai.data.CacheDataset(dataset, rotation_transf)"
      ],
      "metadata": {
        "id": "DXf1iIz19n4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is used to flip the original images and masks to have even more data."
      ],
      "metadata": {
        "id": "6Vb01dMx0N6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_flip(dataset):\n",
        "    masks=[]\n",
        "    images=[]\n",
        "    metadata=[]\n",
        "    num=None # check whether the index is repeated or not per iteration\n",
        "    num2=[]\n",
        "    # Load sample\n",
        "    for element in range(len(dataset)):\n",
        "\n",
        "      sample_dict = dataset[element]\n",
        "\n",
        "      # Add channels\n",
        "      add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask']) # Initialize the transform\n",
        "      channels_sample_dict = add_channels_transform(sample_dict) # Apply the transform\n",
        "      print(\"Size of the image before AddChanneld transform\", sample_dict[\"img\"].shape)\n",
        "      print(\"Size of the image after AddChanneld transform\", channels_sample_dict[\"img\"].shape)\n",
        "\n",
        "\n",
        "\n",
        "      # Random flip\n",
        "      # here we define the keys, the probability that the flip is performed and the axis to flip over\n",
        "      random_flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
        "      # We put a probability of 1 to always flip the image for visualization purposes.\n",
        "      # Please DO NOT DO THAT in the rest of the notebook.\n",
        "      flipped_sample_dict = random_flip_transform(channels_sample_dict)\n",
        "      masks.append(flipped_sample_dict[\"mask\"])\n",
        "      images.append(flipped_sample_dict[\"img\"])\n",
        "      metadata.append(flipped_sample_dict[\"img_meta_dict\"][\"filename_or_obj\"])\n",
        "\n",
        "    return metadata,masks,images"
      ],
      "metadata": {
        "id": "0PmS9sXc0MF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general_data,masks,images=data_flip(dataset)"
      ],
      "metadata": {
        "id": "FnrSVmqdAslo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = '/content/drive/MyDrive/flipped/'\n",
        "for i,element in enumerate(images):\n",
        "  save_nifti(element, masks[i], out, i,general_data)"
      ],
      "metadata": {
        "id": "jfmW36RUA43O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flipped_dataset = read_augmented(out)\n",
        "flipped_dataset = flipped_dataset.to_dict('records')\n",
        "dataset=train_dicts+rotated_dataset+flipped_dataset\n",
        "transformation = LoadAndTransform()\n",
        "dataset = monai.data.CacheDataset(dataset, transformation)"
      ],
      "metadata": {
        "id": "BRt3ppWoBPmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Instantiation**"
      ],
      "metadata": {
        "id": "1qwsKZxG6G3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model architecture utilized for the ACDC challenge was a 2D U-Net, implemented using the MONAI library. The U-Net consisted of multiple encoding and decoding blocks, with a total of four output channels representing different classes. The U-Net architecture employed a series of\n",
        "convolutional layers with increasing channel sizes (8, 16, 32, 64, and 128) and downsampling strides of 2 in each block."
      ],
      "metadata": {
        "id": "LcEd_RYO6QVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXdMXd3e34bt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'The used device is {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Jkputs7OFO"
      },
      "outputs": [],
      "source": [
        "# Define the UNet model architecture\n",
        "model = monai.networks.nets.UNet(\n",
        "    spatial_dims=2,  # Number of spatial dimensions (2D in this case)\n",
        "    in_channels=1,  # Number of input channels\n",
        "    out_channels=4,  # Number of output channels\n",
        "    channels=(8, 16, 32, 64, 128),  # Number of channels in each layer\n",
        "    strides=(2, 2, 2, 2),  # Strides in each layer for downsampling\n",
        "    num_res_units=2,  # Number of residual units in each layer\n",
        ").to(device)  # Move the model to the specified device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function defined here utilizes the Dice loss, a popular and effective loss function commonly employed in medical image segmentation tasks. Let's explore its characteristics:\n",
        "\n",
        "The DiceLoss function, provided by the MONAI library, is specifically designed for binary or multi-class segmentation tasks. It leverages the concept of the Dice coefficient, which measures the similarity between two sets by evaluating their overlap.\n",
        "\n",
        "In this case, the cost function is configured with the sigmoid=True parameter, indicating that the model's output is expected to be passed through a sigmoid activation function. This is particularly useful when dealing with binary segmentation tasks, where the sigmoid function can be employed to obtain probability-like values within the range of 0 to 1.\n",
        "\n",
        "The batch=True parameter specifies that the loss function will operate on batches of data. This allows for efficient computation and enables the utilization of batch processing capabilities provided by deep learning frameworks.\n",
        "\n",
        "By employing the Dice loss, the cost function aims to minimize the dissimilarity between the predicted segmentation outputs and the ground truth labels. It accomplishes this by encouraging high overlap and agreement between the predicted and ground truth segmentations, resulting in more accurate and reliable segmentation models."
      ],
      "metadata": {
        "id": "5mxUXPm-7Lkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv9K_i3F7WMU"
      },
      "outputs": [],
      "source": [
        "cost_function =  monai.losses.DiceLoss(sigmoid=True, batch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SGD (Stochastic Gradient Descent) optimizer is a widely used optimization algorithm in deep learning. It plays a crucial role in training neural networks by iteratively adjusting the model's parameters to minimize the loss function. Let's explore its characteristics:\n",
        "\n",
        "SGD operates on the principle of gradient descent, which involves updating the model's parameters in the direction of the steepest descent of the loss function. It does so by calculating the gradients of the parameters with respect to the loss and taking small steps towards minimizing the loss."
      ],
      "metadata": {
        "id": "oKXHJtv27eUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXCOuMuKB42x"
      },
      "outputs": [],
      "source": [
        "# Set the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set some parameters\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "\n",
        "# Instantiate ResNet18\n",
        "net = model\n",
        "\n",
        "weight_decay=5e-4\n",
        "momentum=0.9\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These code cells contain functions and computations related to evaluating the performance of a model using Dice coefficient as a metric for medical image segmentation."
      ],
      "metadata": {
        "id": "IR6A89S78JWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "metric_fn is initialized as a DiceMetric object from the MONAI library. It is specifically configured to calculate the Dice coefficient, which is a common metric used for evaluating the performance of medical image segmentation models. This particular DiceMetric object is set to exclude the background class from the computation and uses a \"mean\" reduction strategy."
      ],
      "metadata": {
        "id": "0tLdnR7l8TN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_fn = monai.metrics.DiceMetric(include_background=False, reduction=\"mean\")"
      ],
      "metadata": {
        "id": "Kwzc4oDMmF4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DICE_COE is a custom function that calculates the Dice coefficient based on two input masks. It performs the following steps:\n",
        "\n",
        "Computes the intersection of the two masks by multiplying them element-wise.\n",
        "Calculates the sums of the individual masks.\n",
        "Applies the formula (2 * intersection) / (sum of masks) to obtain the Dice coefficient. This formula measures the overlap between the predicted segmentation and the ground truth, providing a measure of similarity between the two."
      ],
      "metadata": {
        "id": "wiNMVaMh8YoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DICE_COE(mask1, mask2):\n",
        "    intersect = np.sum(mask1*mask2)\n",
        "    fsum = np.sum(mask1)\n",
        "    ssum = np.sum(mask2)\n",
        "    dice = (2 * intersect ) / (fsum + ssum)\n",
        "    dice = np.mean(dice)\n",
        "    dice = round(dice, 3)\n",
        "    return dice"
      ],
      "metadata": {
        "id": "hPXGisjqJejq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compute_metric function plays a crucial role in evaluating the performance of a model. It begins by preparing the model for evaluation, ensuring that it operates on the specified device. The function then employs the SlidingWindowInferer technique to perform inference on the input image, enabling efficient processing using a sliding window approach.\n",
        "\n",
        "After obtaining the output predictions from the model, a Softmax operation is applied to generate class probabilities. These probabilities facilitate the calculation of dice coefficients, which measure the overlap or similarity between the predicted labels and the ground truth labels for each class.\n",
        "\n",
        "To assess the model's performance comprehensively, mean dice coefficients are computed for each class. These coefficients provide an average measure of accuracy for individual classes, shedding light on the model's performance in segmenting different objects or regions.\n",
        "\n",
        "The overall estimation is obtained by averaging the mean dice coefficients across all classes. This metric serves as an indicator of the model's overall segmentation accuracy, encapsulating its ability to produce accurate and consistent predictions."
      ],
      "metadata": {
        "id": "erQ0ZsSU8dfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metric(dataloader, model, metric_fn, device):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    inferer = monai.inferers.SlidingWindowInferer(roi_size=[128, 128])\n",
        "    discrete_transform = monai.transforms.AsDiscrete(threshold=0.9)\n",
        "    Softmax = torch.nn.Softmax(dim=1)  # specify the dimension for softmax\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for sample in dataloader:\n",
        "        img = sample['img'].to(device)  # move image to device\n",
        "        mask = sample['mask'].to(device)  # move mask to device\n",
        "        dice_coefficients1 = []\n",
        "        dice_coefficients2 = []\n",
        "        dice_coefficients3 = []\n",
        "        dice_coefficients4 = []\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = inferer(img, network=model)\n",
        "            output = Softmax(output).cpu()  # apply softmax then move to cpu\n",
        "\n",
        "            # Check if output has the expected shape and type (Tensor)\n",
        "            assert isinstance(output, torch.Tensor), \"Output should be a Tensor\"\n",
        "\n",
        "            all_outputs.append(output)\n",
        "            all_labels.append(mask.cpu())  # move mask back to cpu for metric computation\n",
        "\n",
        "            prediction_label1 = np.squeeze(output,axis=0)[0,:,:]\n",
        "            prediction_label2 = np.squeeze(output,axis=0)[1,:,:]\n",
        "            prediction_label3 = np.squeeze(output,axis=0)[2,:,:]\n",
        "            prediction_label4 = np.squeeze(output,axis=0)[3,:,:]\n",
        "\n",
        "            ground_truth1 = np.squeeze(mask,axis=0)[0,:,:]\n",
        "            ground_truth2 = np.squeeze(mask,axis=0)[1,:,:]\n",
        "            ground_truth3 = np.squeeze(mask,axis=0)[2,:,:]\n",
        "            ground_truth4 = np.squeeze(mask,axis=0)[3,:,:]\n",
        "\n",
        "            dice_coef1 = DICE_COE(prediction_label1,ground_truth1)\n",
        "            dice_coef2 = DICE_COE(prediction_label2,ground_truth2)\n",
        "            dice_coef3 = DICE_COE(prediction_label3,ground_truth3)\n",
        "            dice_coef4 = DICE_COE(prediction_label4,ground_truth4)\n",
        "\n",
        "            dice_coefficients1.append(dice_coef1)\n",
        "            dice_coefficients2.append(dice_coef2)\n",
        "            dice_coefficients3.append(dice_coef3)\n",
        "            dice_coefficients4.append(dice_coef4)\n",
        "\n",
        "\n",
        "    # Stack tensors together for metric computation\n",
        "\n",
        "   # mean_dice = metric_fn(torch.stack(all_outputs), torch.stack(all_labels))\n",
        "    mean_dice1 = np.mean(dice_coefficients1)\n",
        "    mean_dice2 = np.mean(dice_coefficients2)\n",
        "    mean_dice3 = np.mean(dice_coefficients3)\n",
        "    mean_dice4 = np.mean(dice_coefficients4)\n",
        "\n",
        "\n",
        "    overall_estimation = np.mean([mean_dice1,mean_dice2,mean_dice3,mean_dice4])\n",
        "    return  overall_estimation,all_outputs, all_labels,mean_dice1,mean_dice2,mean_dice3,mean_dice4"
      ],
      "metadata": {
        "id": "DyVRuSwBJgbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping is a technique used to prevent overfitting and it is based on the idea that the model's performance on the validation set will stop improving after a certain number of training epochs, and continuing to train beyond this point will only lead to overfitting.\n",
        "\n",
        "In this case the monitor is on the perplexity: after three times we see the perplexity increasing we stop the training and get the final value.\n",
        "\n",
        "There are several benefits to using early stopping:\n",
        "\n",
        "It can help prevent overfitting by stopping the training process before the model starts to overfit the training data;\n",
        "It can save time and resources by avoiding the need to train the model for a large number of epochs;\n",
        "It can improve the generalization ability of the model by avoiding the use of models that are overfitted to the training data."
      ],
      "metadata": {
        "id": "tO3UsNao74Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "  \"\"\"\n",
        "    A class for early stopping of training process.\n",
        "\n",
        "    Args:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping. Default: 5\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered. Default: True\n",
        "\n",
        "    Attributes:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping.\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered.\n",
        "        counter (int): Counter for the number of epochs since the last improvement.\n",
        "        best_score (float): Best score seen so far. None by default.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, patience=5, verbose=True):\n",
        "\n",
        "    # set the number of epochs to wait before triggering early stopping\n",
        "    self.patience = patience\n",
        "\n",
        "    # set a flag indicating whether to print messages when early stopping is triggered\n",
        "    self.verbose = verbose\n",
        "\n",
        "    # initialize the counter for the number of epochs since the last improvement\n",
        "    self.counter = 0\n",
        "\n",
        "    # initialize the best score seen so far to None\n",
        "    self.best_score = None\n",
        "\n",
        "  def step(self, val_loss):\n",
        "\n",
        "    \"\"\"\n",
        "        Function to check if early stopping should be triggered.\n",
        "\n",
        "        Args:\n",
        "            val_loss (float): Current validation loss.\n",
        "\n",
        "        Returns:\n",
        "            bool: Whether early stopping should be triggered or not.\n",
        "    \"\"\"\n",
        "\n",
        "    # if this is the first epoch, set the best score to the current validation loss\n",
        "    if self.best_score is None:\n",
        "\n",
        "      self.best_score = val_loss\n",
        "\n",
        "    # if the current validation loss is worse than the best score seen so far\n",
        "    elif val_loss > self.best_score:\n",
        "\n",
        "      # increment the counter\n",
        "      self.counter += 1\n",
        "\n",
        "      # if the counter has reached the patience threshold\n",
        "      if self.counter >= self.patience:\n",
        "\n",
        "        # if verbose is True, print a message indicating that early stopping has been triggered\n",
        "        if self.verbose:\n",
        "\n",
        "          print(f'Early stopping triggered with counter {self.counter} and patience {self.patience}')\n",
        "\n",
        "        # return True to indicate that early stopping should be triggered\n",
        "        return True\n",
        "\n",
        "    # if the current validation loss is better than the best score seen so far\n",
        "\n",
        "    else:\n",
        "\n",
        "      # set the best score to the current validation loss\n",
        "      self.best_score = val_loss\n",
        "\n",
        "      # reset the counter\n",
        "      self.counter = 0\n",
        "\n",
        "    # if the counter has not reached the patience threshold, return False\n",
        "    return False"
      ],
      "metadata": {
        "id": "4m_L1lPkHuEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Step**"
      ],
      "metadata": {
        "id": "e_5N7X4N83ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training_step function is a Python function that represents a training step (training step) of a neural network. The function takes as input a neural network (net), a training set (train_loader), a validation set (valid_loader), an optimizer (optimizer), a cost function (cost_function), and a device (device) on which to run the training (by default \"cuda,\" indicating an NVIDIA GPU).\n",
        "\n",
        "The function first sets the neural network to training mode (net.train()), then runs a training cycle on the training set (for inputs, targets in train_loader:). For each batch of inputs and targets in the training set, the function performs the following steps:\n",
        "\n",
        "Resizing the input image to a specific size. Moving the inputs and targets to the specified device (device). Zeroing the gradients of the optimizer (optimizer.zero_grad()). Forward pass of the neural network (outputs = net(inputs)). Calculation of the cost function (loss = cost_function(outputs, targets)). Backward pass to calculate gradients (loss.backward()). Update of neural network parameters (optimizer.step()). Calculating training accuracy (train_acc) and tracking training loss (train_loss). After running the training cycle on the training set, the function runs a validation cycle on the validation set (for inputs, targets in valid_loader:). For each batch of inputs and targets in the validation set, the function performs steps 1 through 5, calculates the validation accuracy (valid_acc) and tracks the validation loss (valid_loss).\n",
        "\n",
        "Finally, the function returns the average training loss (train_loss), average training accuracy (train_acc), average validation loss (valid_loss) and average validation accuracy (valid_acc) as a tuple. These values can be used to monitor and evaluate the performance of the neural network during training."
      ],
      "metadata": {
        "id": "nptv56AW8_E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(model, data_loader, validation_loader, optimizer, cost_function, device='cuda'):\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    for batch_data in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_data['img'].float().to(device))\n",
        "        samples += outputs.shape[0]\n",
        "        loss = cost_function(outputs, batch_data['mask'].float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    train_loss = epoch_loss / samples\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    samples = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_data in validation_loader:\n",
        "            outputs = model(batch_data['img'].float().to(device))\n",
        "            samples += outputs.shape[0]\n",
        "            loss = cost_function(outputs, batch_data['mask'].float().to(device))\n",
        "            val_loss += loss.item()\n",
        "    val_loss = val_loss / samples\n",
        "\n",
        "    return train_loss, val_loss, model"
      ],
      "metadata": {
        "id": "uleg_E24PbMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dice_metric variable is initialized as a DiceMetric object from the MONAI library. It is configured to calculate the Dice coefficient, a commonly used metric in image segmentation tasks.\n",
        "\n",
        "By setting include_background to True, the dice_metric includes the background class in the computation of the coefficient. This allows for a comprehensive evaluation of the model's segmentation performance across all classes, including the background.\n",
        "\n",
        "The reduction parameter is set to 'mean', indicating that the computed Dice coefficients for each class will be averaged to obtain an overall mean coefficient. This provides a concise and interpretable measure of the model's segmentation accuracy, taking into account all classes and their contributions to the final result.\n",
        "\n",
        "Using the dice_metric, one can conveniently assess the model's performance in terms of segmentation accuracy, accounting for both foreground and background classes. This enables a comprehensive evaluation of the model's ability to accurately delineate regions of interest in the given images."
      ],
      "metadata": {
        "id": "nMdryySi9seB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dice_metric = monai.metrics.DiceMetric(include_background=True, reduction='mean')"
      ],
      "metadata": {
        "id": "vaIf6NZOGaee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code is a Python function named \"main\" that implements a complete iteration of a neural network training process for classification. The function takes as input the training configuration parameters such as the number of epochs, batch size, learning rate, weight decay, momentum, training data loader, validation data loader, optimizer and cost function.\n",
        "\n",
        "Within the function, an Early Stopping criterion is created, which is used to stop training when the validation loss does not improve for a certain number of epochs. Next, the function iterates over the specified number of epochs and at each iteration calls the training_step function to train the network based on the training and validation data loader.\n",
        "\n",
        "During training, the model parameters with the lowest validation loss up to that point are saved. At the end of training, the function uses the saved model parameters to perform the test on the validation data.\n",
        "\n",
        "Finally, the function returns the training and validation losses and accuracies as output. The code also provides the ability to save the trained model to disk for later use."
      ],
      "metadata": {
        "id": "QRDGVSej9JdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size,\n",
        "         device,\n",
        "         learning_rate,\n",
        "         weight_decay,\n",
        "         momentum,\n",
        "         epochs,\n",
        "         train_loader,\n",
        "         validation_loader,\n",
        "         test_loader,\n",
        "         model,\n",
        "         optimizer,\n",
        "         cost_function,\n",
        "         metric_function,\n",
        "         patience):\n",
        "\n",
        "    model_save_path = \"/content/drive/MyDrive/best_model.pth\"\n",
        "\n",
        "    # List to store outputs and labels for testing phase\n",
        "    outputs_test = []\n",
        "    labels_test = []\n",
        "\n",
        "    # Create an instance of the EarlyStopping class\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    # Track loss over each epoch\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    best_loss = 10e6\n",
        "\n",
        "    # Loop over each epoch\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # Training\n",
        "        train_loss, val_loss, model = training_step(model, train_loader, validation_loader, optimizer, cost_function, device)\n",
        "        training_losses.append(train_loss)\n",
        "        validation_losses.append(val_loss)\n",
        "\n",
        "        # Check if early stopping should be triggered\n",
        "        should_stop = early_stopping.step(val_loss)\n",
        "\n",
        "        # Save the model if validation loss has improved\n",
        "        if val_loss <= early_stopping.best_score:\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f'Model saved at {model_save_path}')\n",
        "\n",
        "        # Testing\n",
        "        test_loss, outputs, labels, _, _, _, _ = compute_metric(test_loader, model, metric_function, device)\n",
        "\n",
        "        # Append outputs and labels\n",
        "        outputs_test.append(outputs)\n",
        "        labels_test.append(labels)\n",
        "\n",
        "        # Logging\n",
        "        print(f'Epoch: {e + 1}')\n",
        "        print(f'\\t Training loss: {train_loss:.5f}')\n",
        "        print(f'\\t Validation loss: {val_loss:.5f}')\n",
        "        print(f'\\t Test loss: {test_loss:.5f}')\n",
        "        print('-----------------------------------------------------')\n",
        "\n",
        "        # Break the loop if early stopping is triggered\n",
        "        if should_stop:\n",
        "            break\n",
        "\n",
        "    return outputs_test, labels_test\n",
        "\n",
        "\n",
        "# Example call to main function\n",
        "# Specify all the required parameters before calling.\n",
        "\n",
        "\n",
        "outputs_test, labels_test=main(batch_size, device, learning_rate, weight_decay, momentum, epochs,\n",
        "     dataloader_train, dataloader_validation, dataloader_test, net, optimizer,\n",
        "     cost_function, dice_metric, patience=5)"
      ],
      "metadata": {
        "id": "lAQ-ymyxQKKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Post processing**"
      ],
      "metadata": {
        "id": "vTqJvlzaUJqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the probability matrices are obtained from the U-Net, post-processing\n",
        "steps can enable the calculation of a threshold to binarize the resulting images.\n",
        "A common approach consists in finding the threshold that results in a mask\n",
        "that maximizes the similarity between the resulting mask and a ground truth\n",
        "mask. However, it is not always possible to calculate such threshold since the\n",
        "ground truth mask is not available. We propose an approach that does not rely\n",
        "on ground truth data for the test set, since we assume that this dataset would\n",
        "not be available. The proposed approach consists in computing an area profile\n",
        "of the label of interest in an MRI image. The area profile represents the amount\n",
        "of pixels found to be a foreground for each label with respect to the size of the\n",
        "image. For each slice of the MRI file the area was calculated.\n",
        "A label-specific area profile was computed by obtaining all the areas for each\n",
        "slice per label. The training masks served to create a reference area profile for\n",
        "each label, and the average area profiles was obtained for each label.\n",
        "Furthermore, different thresholds were applied to the probability matrices\n",
        "obtained in the test set. The thresholds ranged from 0 to the maximum pixel\n",
        "intensity found in the probability matrices. Area profiles were computed for each\n",
        "threshold, for each observation of the test set. For each area profile of the test set,\n",
        "the mean square error (MSE) was obtained with respect to it’s corresponding\n",
        "reference area profile. The threshold that corresponded to the minimum MSE\n",
        "was used as final threshold to binarize the probability matrices obtained from\n",
        "the test set.\n",
        "\n",
        "Also in this case it is necessary to create the same folders in the drive in order to run the code (masks and images within testing) as we will see in the different paths recalled."
      ],
      "metadata": {
        "id": "KGJO7yb5Ulsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start again by reading an pre-process the images from the secret test set."
      ],
      "metadata": {
        "id": "1tPum9a0Vvrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58Fj7NR-USVd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def read_png_files(folder_path):\n",
        "    png_files = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.lower().endswith('.png'):\n",
        "            png_files.append(os.path.join(folder_path, file))\n",
        "    return png_files\n",
        "def order_strings(strings):\n",
        "    def custom_sort_key(string):\n",
        "        # Extract the number after 'patient'\n",
        "        patient_num = int(string.split('patient')[1].split('_')[0])\n",
        "        # Extract the number after 'frame'\n",
        "        frame_num = int(string.split('frame')[1].split('_')[0])\n",
        "        # Extract the last two digits of the string\n",
        "        last_two_digits = int(string[-6:-4])\n",
        "        return (patient_num, frame_num, last_two_digits)\n",
        "\n",
        "    return sorted(strings, key=custom_sort_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below the already used LoadAndTransform function."
      ],
      "metadata": {
        "id": "AnRQ7s8lVy-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYUnvF6GUD7q"
      },
      "outputs": [],
      "source": [
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, AddChanneld, RandFlipd, RandRotated, ToTensorD, CenterSpatialCropd,\n",
        "    ScaleIntensityd, AsDiscreted\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "num_classes = 4\n",
        "\n",
        "class LoadAndTransform:\n",
        "    def __init__(self):\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['img', 'mask']),\n",
        "            AddChanneld(keys=['img', 'mask']),\n",
        "            ScaleIntensityd(keys=['img']),\n",
        "            CenterSpatialCropd(keys=['img', 'mask'], roi_size=[128, 128]),\n",
        "            AsDiscreted(keys=['mask'], to_onehot=num_classes, num_classes=num_classes),\n",
        "            ToTensorD(keys=['img', 'mask'])\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self.transform(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the paths have to be changed according to the folder in which the secret test set has been saved in."
      ],
      "metadata": {
        "id": "CJLwM0OoWSEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyYmWm9eUD7q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Use the read_png_files function to get a list of all the .png files in the specified folder for test images\n",
        "path_img_test = read_png_files('/Users/anacordonavila/Desktop/DLMI/group_project/database/testing/images')\n",
        "\n",
        "# Use the read_png_files function to get a list of all the .png files in the specified folder for test masks\n",
        "path_mask_test = read_png_files('/Users/anacordonavila/Desktop/DLMI/group_project/database/testing/masks')\n",
        "\n",
        "# Sort the lists of test image and mask paths using the order_strings function\n",
        "path_img_test = order_strings(path_img_test)\n",
        "path_mask_test= order_strings(path_mask_test)\n",
        "\n",
        "# Create a DataFrame with two columns: 'img' and 'mask', containing the sorted lists of test image and mask paths\n",
        "test_dicts = pd.DataFrame({'img':path_img_test,'mask':path_mask_test})\n",
        "\n",
        "# Convert the DataFrame to a list of dictionaries, where each dictionary represents one row of the DataFrame\n",
        "test_dicts = test_dicts.to_dict('records')\n",
        "\n",
        "# Create an instance of the LoadAndTransform class\n",
        "test_transform=LoadAndTransform()\n",
        "\n",
        "# Create a CacheDataset using the list of dictionaries and the transform object\n",
        "test_dataset = monai.data.CacheDataset(test_dicts, test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The count_patient_repetitions function takes a list of file paths as an argument and returns a dictionary that counts the number of times each combination of patient ID and frame number appears in the list.\n",
        "\n",
        "For each file path in the input list, the function extracts the patient ID and frame number from the file name by splitting the file path on the “/” character to get the file name, then splitting the file name on the “_frame” string to get the patient ID and frame number. The patient ID and frame number are then combined into a unique key by concatenating them with an underscore.\n",
        "\n",
        "The function then updates the count for this key in the patient_frame_counts dictionary by using the get method to retrieve the current count for the key (or 0 if it doesn’t exist yet) and adding 1 to it. This process is repeated for all file paths in the input list."
      ],
      "metadata": {
        "id": "zUwS-ri-W0zL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-twaCy3UD7r"
      },
      "outputs": [],
      "source": [
        "def count_patient_repetitions(file_paths):\n",
        "    patient_frame_counts = {}\n",
        "    for file_path in file_paths:\n",
        "        # Extract the patient ID and frame number from the file path\n",
        "        file_name = file_path.split(\"/\")[-1]  # Extract the file name\n",
        "        patient_id, frame = file_name.split(\"_frame\")[0], file_name.split(\"_frame\")[1].split(\"_\")[0]\n",
        "\n",
        "        # Create a unique key combining patient ID and frame number\n",
        "        key = patient_id + \"_\" + frame\n",
        "\n",
        "        # Update the count for the key\n",
        "        patient_frame_counts[key] = patient_frame_counts.get(key, 0) + 1\n",
        "\n",
        "    return patient_frame_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUkOcwGmUD7r"
      },
      "outputs": [],
      "source": [
        "count_patient_repetitions(path_mask_test).values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the dataloader."
      ],
      "metadata": {
        "id": "Vxrh_VQ3WuM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwT28lRLUD7r"
      },
      "outputs": [],
      "source": [
        "dataloader_test = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the model previously discussed."
      ],
      "metadata": {
        "id": "G30nR4rgW4TB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZ1CeULUD7r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state_dict = torch.load(\"/Users/anacordonavila/Desktop/DLMI/group_project/database/results_0.2731249574571848.pth\")\n",
        "# Define your network architecture\n",
        "net = monai.networks.nets.UNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=1,\n",
        "    out_channels=4,\n",
        "    channels=(8, 16, 32, 64, 128),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "net.load_state_dict(state_dict)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = net.to(device)\n",
        "\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, AddChanneld, RandFlip, RandRotate, ToTensorD, CenterSpatialCropd,\n",
        "    ScaleIntensityd, AsDiscrete\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class LoadAndTransform:\n",
        "    def __init__(self):\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['img']),\n",
        "            AddChanneld(keys=['img']),\n",
        "            ScaleIntensityd(keys=['img']),  # normalize intensities of the image\n",
        "            CenterSpatialCropd(keys=['img'], roi_size=[128, 128]),\n",
        "            ToTensorD(keys=['img'])\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        data = {'img': sample['img']}  # Assuming 'img' is the key for the image data\n",
        "        return self.transform(data)\n",
        "\n",
        "\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Create a list to store the predicted masks\n",
        "masks = []\n",
        "\n",
        "# Run inference on the dataset\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader_test:\n",
        "        images = batch[\"img\"].to(device)\n",
        "        outputs=net(images)\n",
        "        masks.append(outputs)\n",
        "\n",
        "# Concatenate the predicted masks into a single tensor\n",
        "masks = torch.cat(masks, dim=0)\n",
        "\n",
        "# You can further process or visualize the predicted masks as needed\n",
        "print(masks.shape)  # Print the shape of the resulting masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBpsuMMdUD7s"
      },
      "outputs": [],
      "source": [
        "masks=masks.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz0KaSwnUD7s"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "shapes=[]\n",
        "img=[]\n",
        "for element in path_img_test:\n",
        "    img.append(cv2.imread(element))\n",
        "    shapes.append(np.shape(cv2.imread(element))[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resize_image function changes the height and width of an input image to the supplied values. It generates a new image with zeros, centers the original image within it, and calculates the necessary padding values to preserve the aspect ratio. Additionally, the function sets the first channel's extra padding values to 1. After that, the resized image is given back."
      ],
      "metadata": {
        "id": "o37fzpKud10k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vdBzYPGUD7t"
      },
      "outputs": [],
      "source": [
        "def resize_image(image, new_height, new_width):\n",
        "    original_height, original_width = image.shape[1:]\n",
        "\n",
        "    # Calculate padding values\n",
        "    pad_top = (new_height - original_height) // 2\n",
        "    pad_bottom = new_height - original_height - pad_top\n",
        "    pad_left = (new_width - original_width) // 2\n",
        "    pad_right = new_width - original_width - pad_left\n",
        "\n",
        "    # Create new image with zeros\n",
        "    resized_image = np.zeros((4, new_height, new_width), dtype=image.dtype)\n",
        "\n",
        "    # Calculate starting and ending indices for original image placement\n",
        "    start_row = pad_top\n",
        "    end_row = pad_top + original_height\n",
        "    start_col = pad_left\n",
        "    end_col = pad_left + original_width\n",
        "\n",
        "    # Assign original image to the center of the new image\n",
        "    resized_image[:, start_row:end_row, start_col:end_col] = image\n",
        "\n",
        "    # Set the new added values to 1 for the first channel\n",
        "    resized_image[0, :pad_top, :] = 1\n",
        "    resized_image[0, end_row:, :] = 1\n",
        "    resized_image[0, :, :pad_left] = 1\n",
        "    resized_image[0, :, end_col:] = 1\n",
        "\n",
        "    return resized_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the quantity of components supplied in the slices list, the code divides a collection of photos and ground truth masks into groups. Utilizing np.cumsum, it determines the cumulative sum of the slices. Then, after repeating this process for each element in the cumulative total, it adds the matching subsets of resized_masks and gt to the lists of images_separated and gt_masks, respectively."
      ],
      "metadata": {
        "id": "ds1Kvzl5eEhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uWEoWkaUD7t"
      },
      "outputs": [],
      "source": [
        "# List of slices representing the number of elements in each group\n",
        "slices=[10, 10, 8, 8, 9, 9, 9, 9, 10, 10, 9, 9, 9, 9, 10, 10, 8, 8, 9, 9, 6, 6, 10, 10, 10, 10, 11, 11, 10, 10, 10, 10, 10, 10, 9, 9, 17, 17, 9, 9, 8, 8, 9, 9, 8, 8, 21, 21, 17, 17, 16, 16, 19, 19, 9, 9, 17, 17, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 10, 10, 10, 10, 9, 9, 9, 9, 16, 16, 20, 20, 10, 10, 8, 8, 10, 10, 8, 8, 10, 10, 9, 9, 15, 15, 8, 8, 10, 10, 8, 8]\n",
        "\n",
        "# Empty lists to store separated images and ground truth masks\n",
        "images_separated=[]\n",
        "gt_masks=[]\n",
        "\n",
        "# Calculate cumulative sum of slices\n",
        "cumulative_sum = np.cumsum(slices)\n",
        "\n",
        "# Loop over the cumulative sum elements\n",
        "for i,element in enumerate(cumulative_sum):\n",
        "    # Check if it is the first element\n",
        "    if i==0:\n",
        "        # Append the corresponding subset of resized_masks and gt to the separated images and gt_masks lists\n",
        "        images_separated.append(resized_masks[0:element])\n",
        "        gt_masks.append(gt[0:element])\n",
        "    else:\n",
        "        # Append the subset from cumulative_sum[i-1] to element to the separated images and gt_masks lists\n",
        "        images_separated.append(resized_masks[cumulative_sum[i-1]:element])\n",
        "        gt_masks.append(gt[cumulative_sum[i-1]:element])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An image and the quantity of labels are inputs for the separate_labels function. It generates an empty image with distinct channels for each label, assigns pixels from that label to the appropriate channel in the separated image, and then deletes the empty image. It then returns the divided image, where each channel stands for a particular label."
      ],
      "metadata": {
        "id": "jePnwq4ceJ5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTDcRRfCUD7t"
      },
      "outputs": [],
      "source": [
        "def separate_labels(image, num_labels):\n",
        "    # Assuming the image has shape (height, width)\n",
        "    height, width,ch = image.shape\n",
        "\n",
        "    # Create an empty image with separate channels for each label\n",
        "    separated_image = np.zeros((num_labels, height, width), dtype=np.uint8)\n",
        "\n",
        "    for label in range(num_labels):\n",
        "        # Set pixels of the corresponding label to the channel of separated_image\n",
        "        separated_image[label] = (image == label).astype(np.uint8)\n",
        "\n",
        "    return separated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNHJBB-FUD7t"
      },
      "outputs": [],
      "source": [
        "# Set the number of labels\n",
        "num_labels = 4\n",
        "\n",
        "# Initialize a list to store the ground truth masks\n",
        "ground_truth=[]\n",
        "\n",
        "# Loop over the patients in the gt_masks list\n",
        "for patient in gt_masks:\n",
        "    # Initialize a list to store the ground truth masks for the current patient\n",
        "    gt1=[]\n",
        "\n",
        "    # Loop over the slices for the current patient\n",
        "    for slices in patient:\n",
        "        # Use the separate_labels function to separate the labels in the current slice and append the result to gt1\n",
        "        gt1.append(separate_labels(slices[:,:,0], num_labels))\n",
        "\n",
        "    # Append the list of ground truth masks for the current patient to the ground_truth list\n",
        "    ground_truth.append(gt1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A0IZ7fAUD7t"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(image1, image2):\n",
        "    # Flatten the images\n",
        "    image1 = image1.flatten()\n",
        "    image2 = image2.flatten()\n",
        "\n",
        "    # Compute the intersection and sum of pixels\n",
        "    intersection = np.sum(image1 * image2)\n",
        "    sum_pixels = np.sum(image1) + np.sum(image2)\n",
        "\n",
        "    # Compute the Dice coefficient\n",
        "    dice = (2.0 * intersection) / (sum_pixels + 1e-6)  # Add a small constant to avoid division by zero\n",
        "\n",
        "    return dice"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the number of slices supplied in the slices list, this code divides a set of images and ground truth masks into groups. It computes the cumulative sum of the slices list and loops through the components of the cumulative sum. It adds the appropriate subsets of masks and gt to the lists images_separated and gt_masks for each element."
      ],
      "metadata": {
        "id": "A5uN_3FzejTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtwfyrGTUD7t"
      },
      "outputs": [],
      "source": [
        "# Initialize a list of the number of slices for each patient\n",
        "slices=[10, 10, 8, 8, 9, 9, 9, 9, 10, 10, 9, 9, 9, 9, 10, 10, 8, 8, 9, 9, 6, 6, 10, 10, 10, 10, 11, 11, 10, 10, 10, 10, 10, 10, 9, 9, 17, 17, 9, 9, 8, 8, 9, 9, 8, 8, 21, 21, 17, 17,16 ,16 ,19 ,19 ,9 ,9 ,17 ,17 ,10 ,10 ,10 ,10 ,10 ,10 ,10 ,10 ,11 ,11 ,10 ,10 ,10 ,10 ,9 ,9 ,9 ,9 ,16 ,16 ,20 ,20 ,10 ,10 ,8 ,8 ,10 ,10 ,8 ,8 ,10 ,10 ,9 ,9 ,15 ,15 ,8 ,8]\n",
        "\n",
        "# Initialize lists to store the separated images and ground truth masks\n",
        "images_separated=[]\n",
        "gt_masks=[]\n",
        "\n",
        "# Calculate the cumulative sum of the slices list\n",
        "cumulative_sum = np.cumsum(slices)\n",
        "\n",
        "# Loop over the elements in the cumulative_sum list\n",
        "for i,element in enumerate(cumulative_sum):\n",
        "    # If this is the first element in the list\n",
        "    if i==0:\n",
        "        # Append the first element slices to the images_separated and gt_masks lists\n",
        "        images_separated.append(masks[0:element])\n",
        "        gt_masks.append(gt[0:element])\n",
        "    else:\n",
        "        # Append the slices from the previous element to the current element to the images_separated and gt_masks lists\n",
        "        images_separated.append(masks[cumulative_sum[i-1]:element])\n",
        "        gt_masks.append(gt[cumulative_sum[i-1]:element])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code determines the regions for every image in a collection of distinct images. To record the areas for each patient, it initializes empty lists (patients_areas1, patients_areas2, patients_areas3)."
      ],
      "metadata": {
        "id": "WV5fZGJxelNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5MPDjDqUD7u"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the areas for each patient\n",
        "patients_areas1=[]\n",
        "patients_areas2=[]\n",
        "patients_areas3=[]\n",
        "\n",
        "# Loop over the images in the images_separated list\n",
        "for indx in range(len(images_separated)):\n",
        "    # Get the shape of the current image\n",
        "    num,d1,d2,ch=np.shape(images_separated[indx])\n",
        "\n",
        "    # Initialize lists to store the areas for each image\n",
        "    area_1=[]\n",
        "    area_2=[]\n",
        "    area_3=[]\n",
        "\n",
        "    # Loop over the number of images\n",
        "    for i in range(num):\n",
        "        # Create an array of threshold values from 0 to the maximum value in the current image, with 20 evenly spaced values\n",
        "        thresholds=np.linspace(0,max(images_separated[idx],20)\n",
        "\n",
        "        # Initialize lists to store the areas for each threshold value\n",
        "        specific_th1=[]\n",
        "        specific_th2=[]\n",
        "        specific_th3=[]\n",
        "\n",
        "        # Loop over the threshold values\n",
        "        for th in thresholds:\n",
        "            # Calculate the unique values and their counts in the current image, thresholded by th\n",
        "            values, counts = np.unique (images_separated[indx][i][:,:,1]>th.flatten(), return_counts=True)\n",
        "\n",
        "            # Get the height and width of the current image\n",
        "            height,width=np.shape(images_separated[indx][i][:,:,1])\n",
        "\n",
        "            # Calculate the ratio of counts to total pixels in the image\n",
        "            area_ratio1 = (counts / (height*width))\n",
        "\n",
        "            # Subtract the first element of area_ratio1 from 1 and append it to specific_th1\n",
        "            area_ratio1= 1-area_ratio1[0]\n",
        "            specific_th1.append(area_ratio1)\n",
        "\n",
        "            # Repeat the above process for the second and third channels of the current image, appending to specific_th2 and specific_th3 respectively\n",
        "            values, counts = np.unique (images_separated[indx][i][:,:,2]>th.flatten(), return_counts=True)\n",
        "            height,width=np.shape(images_separated[indx][i][:,:,2])\n",
        "            area_ratio1 = (counts / (height*width))\n",
        "            area_ratio1= 1-area_ratio1[0]\n",
        "            specific_th2.append(area_ratio1)\n",
        "\n",
        "            values, counts = np.unique (images_separated[indx][i][:,:,3]>th.flatten(), return_counts=True)\n",
        "            height,width=np.shape(images_separated[indx][i][:,:,3])\n",
        "            area_ratio1 = (counts / (height*width))\n",
        "            area_ratio1= 1-area_ratio1[0]\n",
        "            specific_th3.append(area_ratio1)\n",
        "\n",
        "        # Append the lists of areas for each threshold value to the corresponding list for each image\n",
        "        area_1.append(specific_th1)\n",
        "        area_2.append(specific_th2)\n",
        "        area_3.append(specific_th3)\n",
        "\n",
        "    # Append the lists of areas for each image to the corresponding list for each patient\n",
        "    patients_areas1.append(area_1)\n",
        "    patients_areas2.append(area_2)\n",
        "    patients_areas3.append(area_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path needs to be changed."
      ],
      "metadata": {
        "id": "CcWgcsyrfHgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RofVFVGYUD7u"
      },
      "outputs": [],
      "source": [
        "path=order_strings(read_png_files(\"/Users/anacordonavila/Desktop/DLMI/group_project/database/testing/masks\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pozWJ9YYUD7u"
      },
      "outputs": [],
      "source": [
        "slices=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 10, 10, 10, 10, 10, 10, 9, 9, 10, 10, 9, 9, 8, 8, 11, 11, 8, 8, 10, 10, 7, 7, 9, 9, 8, 8, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 13, 13, 8, 8, 7, 7, 8, 8, 9, 9, 10, 10, 6, 6, 9, 9, 12, 12, 9, 9, 8, 8, 9, 9, 9, 9, 8, 8, 7, 7, 10, 10, 10, 10, 8, 8, 7, 7, 8, 8, 9, 9, 9, 9, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 8, 8, 10, 10, 8, 8, 9, 9, 10, 10, 7, 7, 7, 7, 6, 6, 10, 10, 8, 8, 7, 7, 8, 8, 14, 14, 8, 8, 8, 8, 8, 8, 9, 9, 6, 6, 17, 17, 16, 16, 6, 6, 12, 12, 15, 15, 7, 7, 8, 8, 16, 16, 6, 6, 7, 7, 8, 8, 15, 15, 10, 10, 10, 10, 14, 14, 18, 18, 8, 8, 7, 7, 16, 16, 8, 8]\n",
        "gt_masks2=[]\n",
        "cumulative_sum = np.cumsum(slices)\n",
        "for i,element in enumerate(cumulative_sum):\n",
        "    if i==0:\n",
        "        #images_separated.append(resized_masks[0:element])\n",
        "        gt_masks2.append(g_t[0:element])\n",
        "    else:\n",
        "        #images_separated.append(resized_masks[cumulative_sum[i-1]:element])\n",
        "        gt_masks2.append(g_t[cumulative_sum[i-1]:element])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXvKJmsrUD7u"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the areas for each patient\n",
        "patients_areas1_2=[]\n",
        "patients_areas2_2=[]\n",
        "patients_areas3_2=[]\n",
        "\n",
        "# Loop over the masks in the gt_masks2 list\n",
        "for indx in range(len(gt_masks2)):\n",
        "    # Get the shape of the current mask\n",
        "    num,ch,d1,d2=np.shape(gt_masks2[indx])\n",
        "\n",
        "    # Initialize lists to store the areas for each mask\n",
        "    area_1=[]\n",
        "    area_2=[]\n",
        "    area_3=[]\n",
        "\n",
        "    # Loop over the number of masks\n",
        "    for i in range(num):\n",
        "        # Calculate the unique values and their counts in the current mask\n",
        "        values, counts = np.unique (gt_masks2[indx][i][0].flatten(), return_counts=True)\n",
        "\n",
        "        # Get the height and width of the current mask\n",
        "        height,width=np.shape(gt_masks2[indx][i][0])\n",
        "\n",
        "        # Calculate the ratio of counts to total pixels in the mask\n",
        "        area_ratio1 = (counts / (height*width))\n",
        "\n",
        "        # Subtract the first element of area_ratio1 from 1 and append it to area_1\n",
        "        area_ratio1= 1-area_ratio1[0]\n",
        "        area_1.append(area_ratio1)\n",
        "\n",
        "        # Repeat the above process for the second and third channels of the current mask, appending to area_2 and area_3 respectively\n",
        "        values, counts = np.unique (gt_masks2[indx][i][1].flatten(), return_counts=True)\n",
        "        height,width=np.shape(gt_masks2[indx][i][1])\n",
        "        area_ratio2 = (counts / (height*width))\n",
        "        area_ratio2= 1-area_ratio2[0]\n",
        "        area_2.append(area_ratio2)\n",
        "\n",
        "        values, counts = np.unique (gt_masks2[indx][i][2].flatten(), return_counts=True)\n",
        "        height,width=np.shape(gt_masks2[indx][i][2])\n",
        "        area_ratio3 = (counts / (height*width))\n",
        "        area_ratio3= 1-area_ratio3[0]\n",
        "        area_3.append(area_ratio3)\n",
        "\n",
        "    # Append the lists of areas for each mask to the corresponding list for each patient\n",
        "    patients_areas1_2.append(area_1)\n",
        "    patients_areas2_2.append(area_2)\n",
        "    patients_areas3_2.append(area_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To store areas for each patient, this code initializes three empty lists (a1, a2, and a3). Then, after performing a loop over the items in the three lists (patients_areas1_2, patients_areas2_2, and patients_areas3_2), the following actions are taken:\n",
        "\n",
        "It enters the if statement if the length of the current element is 10.\n",
        "\n",
        "Depending on the list being processed, it appends the current element to a1 (in the first loop), a2 (in the second loop), or a3 (in the third loop)."
      ],
      "metadata": {
        "id": "c7Dx-TjTdT5p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD_VaEb2UD7u"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the areas for each patient\n",
        "a1=[]\n",
        "a2=[]\n",
        "a3=[]\n",
        "\n",
        "# Loop over the elements in the patients_areas1_2 list\n",
        "for element in patients_areas1_2:\n",
        "    # If the length of the current element is 10\n",
        "    if len(element)==10:\n",
        "        # Append the element to the a1 list\n",
        "        a1.append(element)\n",
        "\n",
        "# Repeat the above process for the patients_areas2_2 and patients_areas3_2 lists, appending to the a2 and a3 lists respectively\n",
        "for element in patients_areas2_2:\n",
        "    if len(element)==10:\n",
        "        a2.append(element)\n",
        "\n",
        "for element in patients_areas3_2:\n",
        "    if len(element)==10:\n",
        "        a3.append(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function average_lists, which is defined in this code, determines the average of matching elements from a list of sublists. The input list is represented by the function's sole input parameter, lst. Here is a list of the code's functions:\n",
        "\n",
        "\n",
        "\n",
        "The variable num_lists is set to the number of sublists in the lst.\n",
        "\n",
        "\n",
        "\n",
        "It determines the longest sublist in lst and stores it in the variable max_len.\n",
        "\n",
        "\n",
        "It creates an empty list called result and initializes it to hold the average values.\n",
        "\n",
        "It iterates through the exclusive range from 0 to max_len.\n",
        "\n",
        "The variables total and count are initialized inside the loop to keep track of the sum and the quantity of elements at the current index i.\n",
        "\n",
        "The entire lst's sublists are looped over.\n",
        "\n",
        "The element at index i is added to total and the count is increased if the index i is inside the boundaries of the current sublist.\n",
        "\n",
        "\n",
        "The average of the elements at index i is calculated by dividing total by count and is added to the result list if count is larger than 0."
      ],
      "metadata": {
        "id": "KRflIC-5dIkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRh2yHI6UD7u"
      },
      "outputs": [],
      "source": [
        "def average_lists(lst):\n",
        "    # Get the number of lists in lst\n",
        "    num_lists = len(lst)\n",
        "\n",
        "    # Get the maximum length of the sublists in lst\n",
        "    max_len = max(len(sublist) for sublist in lst)\n",
        "\n",
        "    # Initialize a list to store the result\n",
        "    result = []\n",
        "\n",
        "    # Loop over the range from 0 to max_len\n",
        "    for i in range(max_len):\n",
        "        # Initialize variables to store the total and count of elements at index i\n",
        "        total = 0\n",
        "        count = 0\n",
        "\n",
        "        # Loop over the sublists in lst\n",
        "        for sub_list in lst:\n",
        "            # If index i is within the bounds of the current sublist\n",
        "            if i < len(sub_list):\n",
        "                # Add the element at index i to the total and increment the count\n",
        "                total += sub_list[i]\n",
        "                count += 1\n",
        "\n",
        "        # If count is greater than 0, append the average of the elements at index i to the result list\n",
        "        if count > 0:\n",
        "            result.append(total / count)\n",
        "\n",
        "    # Return the result list\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_DCkQ9cUD7u"
      },
      "outputs": [],
      "source": [
        "# Calculate the average of the lists in a1 and store the result in reference1\n",
        "reference1=average_lists(a1)\n",
        "\n",
        "# Calculate the average of the lists in a2 and store the result in reference2\n",
        "reference2=average_lists(a2)\n",
        "\n",
        "# Calculate the average of the lists in a3 and store the result in reference3\n",
        "reference3=average_lists(a3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function find_min_mse_list returns the index of the list inside the list of lists that has the least mean squared error (MSE) in relation to the reference list and accepts a reference list and a list of lists as inputs. The MSE between each list and the reference list is calculated, and the minimum MSE and its index are tracked as it loops across the lists in the list of lists.\n",
        "\n",
        "\n",
        "An image, a new height, and a new width are passed as parameters to the resize_image function, which returns a new image with the new height and width. The new image is created by centering the previous image and filling any remaining space with zeros. The function determines the padding values for each side of the image, creates a new image that is entirely made of zeros, determines the starting and ending indices for positioning the original image in the center of the new image, places the original image there, and sets any additional values in the first channel to 1."
      ],
      "metadata": {
        "id": "E-UGmwM-ZZs8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZUmnfKMUD7v"
      },
      "outputs": [],
      "source": [
        "def find_min_mse_list(reference_list, list_of_lists):\n",
        "    min_mse = np.inf\n",
        "    min_mse_index = None\n",
        "\n",
        "    for i, current_list in enumerate(list_of_lists):\n",
        "        mse = np.mean((np.array(current_list) - np.array(reference_list)) ** 2)\n",
        "        if mse < min_mse:\n",
        "            min_mse = mse\n",
        "            min_mse_index = i\n",
        "\n",
        "    return min_mse_index,min_mse\n",
        "\n",
        "def resize_image(image, new_height, new_width):\n",
        "    original_height, original_width = image.shape\n",
        "\n",
        "    # Calculate padding values\n",
        "    pad_top = (new_height - original_height) // 2\n",
        "    pad_bottom = new_height - original_height - pad_top\n",
        "    pad_left = (new_width - original_width) // 2\n",
        "    pad_right = new_width - original_width - pad_left\n",
        "\n",
        "    # Create new image with zeros\n",
        "    resized_image = np.zeros((4, new_height, new_width), dtype=image.dtype)\n",
        "\n",
        "    # Calculate starting and ending indices for original image placement\n",
        "    start_row = pad_top\n",
        "    end_row = pad_top + original_height\n",
        "    start_col = pad_left\n",
        "    end_col = pad_left + original_width\n",
        "\n",
        "    # Assign original image to the center of the new image\n",
        "    resized_image[:, start_row:end_row, start_col:end_col] = image\n",
        "\n",
        "    # Set the new added values to 1 for the first channel\n",
        "    resized_image[0, :pad_top, :] = 1\n",
        "    resized_image[0, end_row:, :] = 1\n",
        "    resized_image[0, :, :pad_left] = 1\n",
        "    resized_image[0, :, end_col:] = 1\n",
        "\n",
        "    return resized_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code scans photos and estimates mean squared errors (MSE) for distinct elements. New_images_label1 and mses1 are two empty lists that are initialized. Then, after performing actions such as obtaining photos, locating the minimal MSE, and figuring out the design of ground truth masks, it loops over the elements again. However, the offered code snippet is incomplete and does not include further explanation of what happens after locating the minimum MSE."
      ],
      "metadata": {
        "id": "mqQRHfl3cwEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4C7z8I4UD7v"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the new images and mean squared errors\n",
        "new_images_label1=[]\n",
        "mses1=[]\n",
        "\n",
        "# Loop over the elements in the patients_areas1 list\n",
        "for i,element in enumerate(patients_areas1):\n",
        "    # Initialize a list to store the mean squared errors for the current element\n",
        "    mse=[]\n",
        "\n",
        "    # If the length of the current element is 10\n",
        "    if len(element)==10:\n",
        "        # Get the images for the current patient\n",
        "        img2 = images_separated[i]\n",
        "\n",
        "        # Initialize a list to store the new images for the current patient\n",
        "        imgs=[]\n",
        "\n",
        "        # Get the shape of the ground truth masks for the current patient\n",
        "        s,new_height,new_width,ch=np.shape(gt_masks[i])\n",
        "\n",
        "        # Use the find_min_mse_list function to find the index of the list in reorganize_lists(element) that has the minimum MSE with respect to reference2\n",
        "        x,m=find_min_mse_list(reference2,reorganize_lists(element))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program analyzes photos and computes mean squared errors (MSE) for various components. New_images_label2 and mses2 are two empty lists that are initialized. Then, it runs over the items and carries out a number of activities, such as extracting photos, figuring out a threshold, locating the minimal MSE, resizing images, and producing new binary images. The updated pictures and MSE scores are then added to the appropriate output lists."
      ],
      "metadata": {
        "id": "EDTHPdyGcgLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER98W7DTUD7v"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the new images and mean squared errors\n",
        "new_images_label2=[]\n",
        "mses2=[]\n",
        "\n",
        "# Loop over the elements in the patients_areas2 list\n",
        "for i,element in enumerate(patients_areas2):\n",
        "    # Initialize a list to store the mean squared errors for the current element\n",
        "    mse=[]\n",
        "\n",
        "    # If the length of the current element is 10\n",
        "    if len(element)==10:\n",
        "        # Get the images for the current patient\n",
        "        img2 = images_separated[i]\n",
        "\n",
        "        # Initialize a list to store the new images for the current patient\n",
        "        imgs=[]\n",
        "\n",
        "        # Get the shape of the ground truth masks for the current patient\n",
        "        s,new_height,new_width,ch=np.shape(gt_masks[i])\n",
        "\n",
        "        # Use the find_min_mse_list function to find the index of the list in reorganize_lists(element) that has the minimum MSE with respect to reference2\n",
        "        x,m=find_min_mse_list(reference2,reorganize_lists(element))\n",
        "\n",
        "        # Get the threshold value at index x\n",
        "        th=thresholds[x]\n",
        "\n",
        "        # Append m to the mse list\n",
        "        mse.append(m)\n",
        "\n",
        "        # Loop over the slices in img2\n",
        "        for slices in img2:\n",
        "            # Resize the current slice and threshold it by th, then append it to imgs\n",
        "            sl = resize_image(slices[2],new_height,new_width)\n",
        "            imgs.append(sl[0,:,:]>th)\n",
        "\n",
        "        # Append imgs to new_images_label2\n",
        "        new_images_label2.append(imgs)\n",
        "\n",
        "    # Append mse to mses2\n",
        "    mses2.append(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program analyzes photos and computes mean squared errors (MSE) for various list elements. Two empty lists, new_images_label3 and mses3, are initialized. Then, it runs through the patients_areas3 list's components. It carries out a number of processes for each element, such as retrieving the photos for the current patient, calculating a threshold based on the minimum MSE, resizing photographs, and producing new binary images, as well as finding the minimum MSE between a reference list and the element in question. The updated pictures and MSE scores are then added to the appropriate output lists."
      ],
      "metadata": {
        "id": "_j7hu9lqcLmG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRN7s4EUD7v"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the new images and mean squared errors\n",
        "new_images_label3=[]\n",
        "mses3=[]\n",
        "\n",
        "# Loop over the elements in the patients_areas3 list\n",
        "for i,element in enumerate(patients_areas3):\n",
        "    # Initialize a list to store the mean squared errors for the current element\n",
        "    mse=[]\n",
        "\n",
        "    # If the length of the current element is 10\n",
        "    if len(element)==10:\n",
        "        # Get the images for the current patient\n",
        "        img2 = images_separated[i]\n",
        "\n",
        "        # Initialize a list to store the new images for the current patient\n",
        "        imgs=[]\n",
        "\n",
        "        # Get the shape of the ground truth masks for the current patient\n",
        "        s,new_height,new_width,ch=np.shape(gt_masks[i])\n",
        "\n",
        "        # Use the find_min_mse_list function to find the index of the list in reorganize_lists(element) that has the minimum MSE with respect to reference2\n",
        "        x,m=find_min_mse_list(reference2,reorganize_lists(element))\n",
        "\n",
        "        # Get the threshold value at index x\n",
        "        th=thresholds[x]\n",
        "\n",
        "        # Append m to the mse list\n",
        "        mse.append(m)\n",
        "\n",
        "        # Loop over the slices in img2\n",
        "        for slices in img2:\n",
        "            # Resize the current slice and threshold it by th, then append it to imgs\n",
        "            sl = resize_image(slices[3],new_height,new_width)\n",
        "            imgs.append(sl[0,:,:]>th)\n",
        "\n",
        "        # Append imgs to new_images_label3\n",
        "        new_images_label3.append(imgs)\n",
        "\n",
        "    # Append mse to mses3\n",
        "    mses3.append(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm determines the Dice coefficients between each patient's fresh photos and their ground truth masks. This is accomplished by iterating through the patients in the new_images_label1, new_images_label2, and new_images_label3 lists, then iterating through the slices for each patient and calculating the Dice coefficient between the current slice and the corresponding ground truth mask using the dice_coefficient function. The dices1, dices2, and dices3 lists, respectively, provide the Dice coefficients. The Dice coefficient, which ranges from 0 (no overlap) to 1 (perfect overlap), is a metric for comparing two sets. It is applied here to gauge how closely the fresh photos resemble the ground truth masks."
      ],
      "metadata": {
        "id": "LSvoruT4asDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwdPnlpAUD7v"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the Dice coefficients for each patient\n",
        "dices1=[]\n",
        "dices2=[]\n",
        "dices3=[]\n",
        "\n",
        "# Loop over the patients in new_images_label1\n",
        "for idx, patient in enumerate(new_images_label1):\n",
        "    # Initialize a list to store the Dice coefficients for the current patient\n",
        "    dices_2=[]\n",
        "\n",
        "    # Loop over the slices for the current patient\n",
        "    for idx2, slices in enumerate(patient):\n",
        "        # Get the corresponding ground truth mask\n",
        "        slices2=gt1[idx][idx2][:,:,0]\n",
        "\n",
        "        # Calculate the Dice coefficient between slices and slices2, then append it to dices_2\n",
        "        d=dice_coefficient(np.flip(slices),slices2)\n",
        "        dices_2.append(d)\n",
        "\n",
        "    # Append dices_2 to dices1\n",
        "    dices1.append((dices_2))\n",
        "\n",
        "# Repeat the above process for new_images_label2 and new_images_label3, appending to dices2 and dices3 respectively\n",
        "for idx, patient in enumerate(new_images_label2):\n",
        "    dices_2=[]\n",
        "    for idx2, slices in enumerate(patient):\n",
        "        slices2=gt2[idx][idx2][:,:,0]\n",
        "        d=dice_coefficient(slices,slices2)\n",
        "        dices_2.append(d)\n",
        "    dices2.append((dices_2))\n",
        "\n",
        "for idx, patient in enumerate(new_images_label3):\n",
        "    dices_2=[]\n",
        "    for idx2, slices in enumerate(patient):\n",
        "        slices2=gt3[idx][idx2][:,:,0]\n",
        "        d=dice_coefficient(slices,slices2)\n",
        "        dices_2.append(d)\n",
        "    dices3.append(dices_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing Secret Dataset**"
      ],
      "metadata": {
        "id": "Yyc6WhUSGnwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This snipped code has been used to test the secret test we were given."
      ],
      "metadata": {
        "id": "V9GTa6Mr3Jua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by reading the images of the new patients."
      ],
      "metadata": {
        "id": "FVanhNNA3P5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data2(path,mode=\"training\",label=\"images\"):\n",
        "    path_img=[]\n",
        "    total= path+mode+\"/\"+label\n",
        "    files=os.listdir(total)\n",
        "    paths=[]\n",
        "    for element in files:\n",
        "      paths.append(total+'/'+element)\n",
        "    paths.sort()\n",
        "    print(paths)\n",
        "\n",
        "    for element in (paths):\n",
        "            path_slice = element+\"/\"\n",
        "            file_list = glob.glob(glob.escape(path_slice[0]) + \"*.gz\")\n",
        "            sorted_file_list = sorted(file_list)\n",
        "            path_img.append(element)\n",
        "\n",
        "    return path_img"
      ],
      "metadata": {
        "id": "mIDXfBHb1QJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/database/'\n",
        "path_img = read_data2(path,mode=\"testin2\",label=\"images\")"
      ],
      "metadata": {
        "id": "dghpVkhr1Q1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the weights of the network trained in the training phase\n",
        "state_dict = torch.load(\"/content/drive/MyDrive/results_0.680.pth\")\n",
        "\n",
        "# Define your network architecture\n",
        "net = monai.networks.nets.UNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=1,\n",
        "    out_channels=4,\n",
        "    channels=(8, 16, 32, 64, 128),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "net.load_state_dict(state_dict)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = net.to(device)\n",
        "\n",
        "from monai.transforms import (\n",
        "    Compose, LoadImage, AddChannel, RandFlip, RandRotate, ToTensor, CenterSpatialCropd,\n",
        "    ScaleIntensity, AsDiscrete\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class LoadAndTransform:\n",
        "    def __init__(self):\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['img']),\n",
        "            AddChanneld(keys=['img']),\n",
        "            ScaleIntensityd(keys=['img']),  # normalize intensities of the image\n",
        "            CenterSpatialCropd(keys=['img'], roi_size=[128, 128]),\n",
        "            ToTensorD(keys=['img'])\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        data = {'img': sample['img']}  # Assuming 'img' is the key for the image data\n",
        "        return self.transform(data)\n",
        "\n",
        "\n",
        "train_transform = LoadAndTransform()\n",
        "train_data = pd.DataFrame({'img':path_img})\n",
        "\n",
        "train_data = train_data.to_dict('records')\n",
        "train_dataset = monai.data.CacheDataset(train_data, train_transform)\n",
        "dataloader_train = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Create a list to store the predicted masks\n",
        "masks = []\n",
        "\n",
        "# Run inference on the dataset\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader_train:\n",
        "        images = batch[\"img\"].to(device)\n",
        "        outputs=net(images)\n",
        "        masks.append(outputs)\n",
        "\n",
        "# Concatenate the predicted masks into a single tensor\n",
        "masks = torch.cat(masks, dim=0)"
      ],
      "metadata": {
        "id": "Z1JlTuEH1Q4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loops through each element in the path_img list and checks if a specific condition is met. The condition compares a substring of the element's filename (element[47:-10]) with the string 'patient152_frame01'. If the condition is true, it means that the element corresponds to a specific slice identified as \"patient152_frame01\". In such cases, the slices variable is incremented by 1, effectively counting the number of slices that meet the condition."
      ],
      "metadata": {
        "id": "VdRWgV7J35vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slices = 0  # Initialize a variable to count the number of slices\n",
        "for element in path_img:  # Iterate through each element in the path_img list\n",
        "  if element[47:-10] == 'patient152_frame01':  # Check if the specific condition is met\n",
        "    slices = slices + 1  # Increment the slice count by 1"
      ],
      "metadata": {
        "id": "eVcttlov1Q8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet reads an image, retrieves its shape, and also obtains the shape information of a masks array."
      ],
      "metadata": {
        "id": "6IryewVM4Jyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = sitk.ReadImage(path_img[14]) # make sure the index you are setting corresponds to the subject we want to get the size from\n",
        "image = sitk.GetArrayFromImage(image)\n",
        "s1,s2 = np.shape(image) # get desired shape\n",
        "image,channels,s1_1,s2_1=np.shape(masks)\n",
        "shape = (slices, channels,s1,s2)  # Specify the desired shape of the array\n",
        "print(shape)"
      ],
      "metadata": {
        "id": "16cWw4b-1b15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scope of this code is to create an empty NumPy array of a specific shape and populate it with masks from the masks array. It then saves the resulting array as a NIfTI file."
      ],
      "metadata": {
        "id": "sNTtgpoF4U_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_array = np.empty(shape)\n",
        "\n",
        "# get sizes to reseize\n",
        "m,n,s1_target,s2_target=(np.shape(empty_array))\n",
        "count=0\n",
        "for element in range(len(path_img)):\n",
        "  if path_img[element][47:-10]=='patient152_frame01':\n",
        "      mask=masks[count,:,:,:].detach().cpu().numpy()\n",
        "      # threshold that can be implemented we can modify to get better results\n",
        "\n",
        "      mask[0,:,:]=mask[0,:,:]<0\n",
        "      mask[1,:,:]=mask[1,:,:]>0\n",
        "      mask[2,:,:]=mask[2,:,:]>0\n",
        "      mask[3,:,:]=mask[3,:,:]>0\n",
        "\n",
        "     # desired shape for the masks\n",
        "\n",
        "      desired_shape = (s1_target, s2_target)\n",
        "      pad_height = max(0, (desired_shape[0] - 128) // 2)\n",
        "      pad_width = max(0, (desired_shape[1] - 128) // 2)\n",
        "      mask1= np.pad(mask[0,:,:], ((pad_height, pad_height), (pad_width, pad_width)), mode='constant')\n",
        "      mask2 = np.pad(mask[1,:,:], ((pad_height, pad_height), (pad_width, pad_width)), mode='constant')\n",
        "      mask3 = np.pad(mask[2,:,:], ((pad_height, pad_height), (pad_width, pad_width)), mode='constant')\n",
        "      mask4 = np.pad(mask[3,:,:], ((pad_height, pad_height), (pad_width, pad_width)), mode='constant')\n",
        "\n",
        "    # fill in array\n",
        "\n",
        "      empty_array[count,0,:,:]=mask1\n",
        "      empty_array[count,1,:,:]=mask2\n",
        "      empty_array[count,2,:,:]=mask3\n",
        "      empty_array[count,3,:,:]=mask4\n",
        "      count=count+1\n",
        "\n",
        "path1='/content/drive/MyDrive/database/testin2/masks/'\n",
        "path2='patient152_frame01'+\"_seg\"+\".nii.gz\"\n",
        "\n",
        "affine = np.eye(4)\n",
        "nifti_file = nibabel.Nifti1Image(empty_array, affine)\n",
        "nibabel.save(nifti_file, path1+path2)"
      ],
      "metadata": {
        "id": "6njwGM9n1lSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing and visualizing masks obteined."
      ],
      "metadata": {
        "id": "9oQu63wm4XJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = sitk.ReadImage('/content/drive/MyDrive/database/testin2/masks/patient152_frame01_seg.nii.gz')\n",
        "image = sitk.GetArrayFromImage(image)\n",
        "print(np.shape(image))\n",
        "plt.imshow(image[:,:,2,8])"
      ],
      "metadata": {
        "id": "7GiQWybb1lXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification**"
      ],
      "metadata": {
        "id": "nLbKOxeV43FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different typologies of CNNs have been tested for this task. Instead of using the actual images as inputs to the neural network, segmentation masks that emphasize the regions of interest were employed. This made sure that the network focused on the important areas and was not diverted by noise or background information. SqueezeNet, VGG, AlexNet, DenseNet121, and ResNet18 have been tried out. Namely ResNet have been implemented it from scratch with its regularization techniques: dropout, early stopping, batch normalization, and L2 regularization.\n",
        "\n",
        "Due to the limitation of GPU we were not able to train the models properly (particularly ResNet 18 from scatch which is not pre-trained).\n",
        "\n",
        "A future work would be to feed the networks with more information (like weight and height of the patients) in order to make it able to distinguish doubtful situations."
      ],
      "metadata": {
        "id": "kW8waQjY0dxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in segmentation we need first of all to create proper dictionaries, this time reading the labels provided for each patient."
      ],
      "metadata": {
        "id": "8CmwOXO32qm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GBKWSiC482k"
      },
      "outputs": [],
      "source": [
        "def read_data_classification(path, mode=\"training\"):\n",
        "    path_mask=[]\n",
        "    group_labels=[]\n",
        "    paths=glob.glob(data_path + mode + \"/*\")\n",
        "    paths.sort()\n",
        "\n",
        "    # define the group-to-index mapping\n",
        "    group_to_idx = {'NOR': 0, 'MINF': 1, 'DCM': 2, 'HCM': 3, 'RV': 4} # modify this according to your groups\n",
        "\n",
        "    for element in paths:\n",
        "        if (element[-10:-3]) == \"patient\":\n",
        "            path_slice = glob.glob(element + \"/\")\n",
        "            cfg_path = os.path.join(element, \"Info.cfg\")\n",
        "\n",
        "            # read group label from the cfg file\n",
        "            group = None\n",
        "            with open(cfg_path, 'r') as cfg_file:\n",
        "                lines = cfg_file.readlines()\n",
        "                for line in lines:\n",
        "                    if 'Group' in line:\n",
        "                        group = line.split(\":\")[1].strip()\n",
        "                        break\n",
        "\n",
        "            # convert group to integer index\n",
        "            group_idx = group_to_idx[group]\n",
        "\n",
        "            file_list = glob.glob(glob.escape(path_slice[0]) + \"*.gz\")\n",
        "            sorted_file_list = sorted(file_list)\n",
        "            for element in sorted_file_list:\n",
        "                if (element[-9:-7])==\"gt\":\n",
        "                    path_mask.append(element)\n",
        "                    group_labels.append(group_idx)  # assign group index to the mask\n",
        "\n",
        "    dicts = pd.DataFrame({'mask':path_mask, 'label':group_labels })\n",
        "    return dicts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_path = '/content/'\n",
        "\n",
        "# Read data for training\n",
        "train_data_classification = read_data_classification(data_path, mode='training')\n",
        "\n",
        "# Split the training data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data_classification, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the DataFrames to lists of dictionaries\n",
        "train_dicts_classification = train_data.to_dict('records')\n",
        "val_dicts_classification = val_data.to_dict('records')\n",
        "\n",
        "# Read data for testing\n",
        "test_dicts_classification = read_data_classification(data_path, mode='testing')\n",
        "test_dicts_classification = test_dicts_classification.to_dict('records')"
      ],
      "metadata": {
        "id": "0deor5sXhXYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, AddChanneld, RandFlipd, RandRotated, ToTensorD, CenterSpatialCropd, SqueezeDimd\n",
        ")\n",
        "\n",
        "class LoadAndTransform_classification:\n",
        "    def __init__(self):\n",
        "        self.transform = Compose([\n",
        "            LoadImaged(keys=['mask']),  # Load the mask using the specified key\n",
        "            AddChanneld(keys=['mask']),  # Add a channel dimension to the mask\n",
        "            CenterSpatialCropd(keys=['mask'], roi_size=[128, 128, 1]),  # Crop the mask to a specific size\n",
        "            SqueezeDimd(keys=['mask'], dim=-1),  # Remove the last dimension from the mask\n",
        "            ToTensorD(keys=['mask'])  # Convert the mask to a PyTorch tensor\n",
        "        ])\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self.transform(sample)  # Apply the defined transformation to the input sample\n",
        "\n",
        "train_transform = LoadAndTransform_classification()  # Instantiate the LoadAndTransform_classification class for training data\n",
        "train_dataset_classification = monai.data.CacheDataset(train_dicts_classification, train_transform)  # Create a dataset using the training transformation\n",
        "\n",
        "val_transform = LoadAndTransform_classification()  # Instantiate the LoadAndTransform_classification class for validation data\n",
        "val_dataset_classification = monai.data.CacheDataset(val_dicts_classification, val_transform)\n",
        "\n",
        "test_transform = LoadAndTransform_classification()  # Instantiate the LoadAndTransform_classification class for test data\n",
        "test_dataset_classification = monai.data.CacheDataset(test_dicts_classification, test_transform)  # Create a dataset using the test transformation"
      ],
      "metadata": {
        "id": "JCRJRWoPlMyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataloader\n",
        "dataloader_train_classification = DataLoader(dataset=train_dataset_classification, batch_size=16, shuffle=True)\n",
        "dataloader_val_classification = DataLoader(dataset=val_dataset_classification, batch_size=16, shuffle=True)\n",
        "dataloader_test_classification = DataLoader(dataset=test_dataset_classification, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "ePKfGj8Ah3jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first framework used for the purpose of image classification is ResNet 18, here implemented from the scratch.\n",
        "\n",
        "ResNet18 is a convolutional neural network architecture part of the ResNet family of models, which was designed to overcome the problem of vanishing gradients in very deep neural networks. The vanishing gradient problem occurs when gradients become very small during backpropagation, making it difficult to train deep neural networks.\n",
        "\n",
        "ResNet18 is made up of 18 layers, including several residual blocks. Residual blocks are a key feature of the ResNet architecture and are designed to enable the efficient training of very deep neural networks. In a residual block, the input to a layer is added to the output of the layer after passing through one or more convolutional layers. This so-called skip connection enables the network to learn residual functions, which can be easier to optimize than the original functions.\n",
        "\n",
        "The techniques of Droupout as well as L2-normalization are added as regularization techniques in order to prevent the phenomenon of Early Stopping."
      ],
      "metadata": {
        "id": "judWxF9H983J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, inchannel, outchannel, stride=1):\n",
        "\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
        "\n",
        "        # Shortcut connection to add to output\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inchannel != outchannel:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outchannel)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # First convolutional layer\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Second convolutional layer\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Shortcut connection\n",
        "        out = out + self.shortcut(x)\n",
        "\n",
        "        # ReLU activation\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, ResidualBlock, num_classes=12):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.inchannel = 6\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # define first convolution layer\n",
        "\n",
        "            nn.Conv2d(3, 6, kernel_size=(2, 2), stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # define the first residual block layer\n",
        "        self.layer1 = self.make_layer(ResidualBlock, 32, 2, stride=1)\n",
        "\n",
        "        # define the second residual block layer\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 64, 2, stride=2)\n",
        "\n",
        "        # define the third residual block layer\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
        "\n",
        "        # define the fourth residual block layer\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
        "\n",
        "        # define global average pooling layer\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # define the fully connected layer\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "\n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "\n",
        "            # create a residual block layer and append it to layers\n",
        "            layers.append(block(self.inchannel, channels, stride))\n",
        "            self.inchannel = channels\n",
        "\n",
        "        # create a sequential layer using the created residual blocks\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # pass the input through the first convolution layer\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        # pass the input through the first residual block layer\n",
        "        out = self.layer1(out)\n",
        "\n",
        "        # pass the input through the second residual block layer\n",
        "        out = self.layer2(out)\n",
        "\n",
        "        # pass the input through the third residual block layer\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        # pass the input through the fourth residual block layer\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        # perform global average pooling on the output of the last residual block layer\n",
        "        out = self.global_avg_pool(out)\n",
        "\n",
        "        # flatten the output tensor\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # L2 normalization\n",
        "        out = F.normalize(out, p=2, dim=1, eps=1e-12)\n",
        "\n",
        "        # Dropout\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # pass the output tensor through the fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "  return ResNet(ResidualBlock)"
      ],
      "metadata": {
        "id": "FHz4DvyH9P62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code cell we want to test other frameworks imported directly from the PyTorch library. Specifically we use Alexnet, VGG, Squeezenet, and Densenet.\n",
        "\n",
        "AlexNet is a deep convolutional neural network architecture that was proposed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. Some of its most important features include:\n",
        "\n",
        "AlexNet has a total of 8 layers, including 5 convolutional layers, 2 fully connected layers, and 1 softmax layer;\n",
        "It uses the rectified linear unit (ReLU) activation function, which helps to improve training time and reduce the risk of the vanishing gradient problem;\n",
        "AlexNet introduced the concept of overlapping pooling, which means that the pooling regions of adjacent pooling layers overlap instead of being disjoint;\n",
        "The architecture also includes local response normalization (LRN), which helps to normalize the response of adjacent neurons and improves the generalization of the model;\n",
        "AlexNet was trained on the ImageNet dataset, which contains millions of labeled images across 1,000 different categories\n",
        "VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by Karen Simonyan and Andrew Zisserman from the University of Oxford in 2014. The VGG architecture is known for its simplicity and effectiveness in deep learning for computer vision tasks. Some key features of VGG are:\n",
        "\n",
        "VGG has a total of 19 layers, including 16 convolutional layers and 3 fully connected layers;\n",
        "The convolutional layers in VGG are composed of 3x3 filters, which are stacked on top of each other to form deeper representations;\n",
        "The pooling layers in VGG use max pooling with a 2x2 filter size and a stride of 2;\n",
        "VGG uses the rectified linear unit (ReLU) activation function, which helps to improve training time and reduce the risk of the vanishing gradient problem;\n",
        "VGG was also trained on the ImageNet dataset, which contains millions of labeled images across 1,000 different categories.\n",
        "SqueezeNet is a convolutional neural network architecture that was proposed by Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer from the University of California, Berkeley in 2016. The SqueezeNet architecture is known for its small size, which makes it well-suited for resource-constrained environments such as mobile devices and embedded systems. Here are some key features of SqueezeNet:\n",
        "\n",
        "SqueezeNet is a small network with a total of only 23 layers, which is much smaller than other popular architectures such as AlexNet and VGG;\n",
        "It achieves its small size by using a combination of 1x1 convolutional filters and a technique called \"fire modules,\" which are designed to balance the number of filters and the computational cost;\n",
        "SqueezeNet also uses a technique called \"deep compression,\" which reduces the size of the network even further by applying pruning and quantization techDespite its small size, SqueezeNet achieves competitive accuracy on the ImageNet dataset, which contains millions of labeled images across 1,000 different categories.\n",
        "The following function is to use the pre-trained model only to extract image features. This can be useful to save computational time and resources, as there is no need to calculate gradients and update model weights."
      ],
      "metadata": {
        "id": "d9_jpG01-A6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "\n",
        "    \"\"\"\n",
        "    A function that sets the requires_grad attribute of model parameters\n",
        "    to False if feature_extracting is True, and to True otherwise.\n",
        "\n",
        "    Args:\n",
        "    - model: a PyTorch model\n",
        "    - feature_extracting: a boolean value that indicates whether we want\n",
        "      to extract features from the model (True) or fine-tune the model (False)\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # If we're only extracting features, we don't want to update the model parameters\n",
        "    if feature_extracting:\n",
        "        # Loop over all parameters in the model and set their requires_grad attribute to False\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "ymNG1LbbiEhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZTI0aU-jtFF"
      },
      "outputs": [],
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 5\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 256\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 128\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"ResNet18\":\n",
        "        model_ft = ResNet18()\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=False)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather the parameters to be optimized/updated in this run. If we are finetuning we will be updating all parameters. However, if we are doing\n",
        "# feature extract method, we will only update the parameters that we have just initialized, i.e. the parameters with requires_grad is True\n",
        "\n",
        "def update_params_to_learn(model_ft, feature_extract):\n",
        "\n",
        "    \"\"\"\n",
        "    A function that determines which parameters of a PyTorch model should be\n",
        "    updated during training, based on whether we want to fine-tune the model\n",
        "    (feature_extract = False) or extract features (feature_extract = True).\n",
        "\n",
        "    Args:\n",
        "    - model_ft: a PyTorch model\n",
        "    - feature_extract: a boolean value that indicates whether we want to\n",
        "      extract features from the model (True) or fine-tune the model (False)\n",
        "\n",
        "    Returns:\n",
        "    - params_to_update: a list of parameters to be updated during training\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all parameters of the model\n",
        "    params_to_update = model_ft.parameters()\n",
        "\n",
        "    # If we're only extracting features, we only want to update certain parameters\n",
        "    if feature_extract:\n",
        "        # Create an empty list to store the parameters we want to update\n",
        "        params_to_update = []\n",
        "        # Loop over all named parameters in the model and check if requires_grad is True\n",
        "        for name, param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                # If the parameter should be updated, append it to the list\n",
        "                params_to_update.append(param)\n",
        "                print(\"\\t\", name)\n",
        "    # If we're fine-tuning the model, update all parameters with requires_grad=True\n",
        "    else:\n",
        "        for name, param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                print(\"\\t\", name)\n",
        "\n",
        "    return params_to_update"
      ],
      "metadata": {
        "id": "NrlvSL93iEbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here below we can choose which one between the neural networks architectures previously mentioned we want to test, just changing the parameter \"model_name\""
      ],
      "metadata": {
        "id": "vl7vBDp3-Fin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set some parameters\n",
        "epoch = 50\n",
        "learning_rate = 0.001\n",
        "batch_size=1\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "\n",
        "# Set the model we are going to use\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Loss funtion\n",
        "cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "Tk-GZsY3iES8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping is a technique used to prevent overfitting and it is based on the idea that the model's performance on the validation set will stop improving after a certain number of training epochs, and continuing to train beyond this point will only lead to overfitting.\n",
        "\n",
        "In this case the monitor is on the perplexity: after three times we see the perplexity increasing we stop the training and get the final value.\n",
        "\n",
        "There are several benefits to using early stopping:\n",
        "\n",
        "It can help prevent overfitting by stopping the training process before the model starts to overfit the training data;\n",
        "It can save time and resources by avoiding the need to train the model for a large number of epochs;\n",
        "It can improve the generalization ability of the model by avoiding the use of models that are overfitted to the training data."
      ],
      "metadata": {
        "id": "MHqI1CPx-Mwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "  \"\"\"\n",
        "    A class for early stopping of training process.\n",
        "\n",
        "    Args:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping. Default: 5\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered. Default: True\n",
        "\n",
        "    Attributes:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping.\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered.\n",
        "        counter (int): Counter for the number of epochs since the last improvement.\n",
        "        best_score (float): Best score seen so far. None by default.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, patience=5, verbose=True):\n",
        "\n",
        "    # set the number of epochs to wait before triggering early stopping\n",
        "    self.patience = patience\n",
        "\n",
        "    # set a flag indicating whether to print messages when early stopping is triggered\n",
        "    self.verbose = verbose\n",
        "\n",
        "    # initialize the counter for the number of epochs since the last improvement\n",
        "    self.counter = 0\n",
        "\n",
        "    # initialize the best score seen so far to None\n",
        "    self.best_score = None\n",
        "\n",
        "  def step(self, val_loss):\n",
        "\n",
        "    \"\"\"\n",
        "        Function to check if early stopping should be triggered.\n",
        "\n",
        "        Args:\n",
        "            val_loss (float): Current validation loss.\n",
        "\n",
        "        Returns:\n",
        "            bool: Whether early stopping should be triggered or not.\n",
        "    \"\"\"\n",
        "\n",
        "    # if this is the first epoch, set the best score to the current validation loss\n",
        "    if self.best_score is None:\n",
        "\n",
        "      self.best_score = val_loss\n",
        "\n",
        "    # if the current validation loss is worse than the best score seen so far\n",
        "    elif val_loss > self.best_score:\n",
        "\n",
        "      # increment the counter\n",
        "      self.counter += 1\n",
        "\n",
        "      # if the counter has reached the patience threshold\n",
        "      if self.counter >= self.patience:\n",
        "\n",
        "        # if verbose is True, print a message indicating that early stopping has been triggered\n",
        "        if self.verbose:\n",
        "\n",
        "          print(f'Early stopping triggered with counter {self.counter} and patience {self.patience}')\n",
        "\n",
        "        # return True to indicate that early stopping should be triggered\n",
        "        return True\n",
        "\n",
        "    # if the current validation loss is better than the best score seen so far\n",
        "\n",
        "    else:\n",
        "\n",
        "      # set the best score to the current validation loss\n",
        "      self.best_score = val_loss\n",
        "\n",
        "      # reset the counter\n",
        "      self.counter = 0\n",
        "\n",
        "    # if the counter has not reached the patience threshold, return False\n",
        "    return False"
      ],
      "metadata": {
        "id": "dOl7AeXojpsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training_step function is a Python function that represents a training step (training step) of a neural network. The function takes as input a neural network (net), a training set (train_loader), a validation set (valid_loader), an optimizer (optimizer), a cost function (cost_function), and a device (device) on which to run the training (by default \"cuda,\" indicating an NVIDIA GPU).\n",
        "\n",
        "The function first sets the neural network to training mode (net.train()), then runs a training cycle on the training set (for inputs, targets in train_loader:). For each batch of inputs and targets in the training set, the function performs the following steps:\n",
        "\n",
        "Resizing the input image to a specific size. Moving the inputs and targets to the specified device (device). Zeroing the gradients of the optimizer (optimizer.zero_grad()). Forward pass of the neural network (outputs = net(inputs)). Calculation of the cost function (loss = cost_function(outputs, targets)). Backward pass to calculate gradients (loss.backward()). Update of neural network parameters (optimizer.step()). Calculating training accuracy (train_acc) and tracking training loss (train_loss). After running the training cycle on the training set, the function runs a validation cycle on the validation set (for inputs, targets in valid_loader:). For each batch of inputs and targets in the validation set, the function performs steps 1 through 5, calculates the validation accuracy (valid_acc) and tracks the validation loss (valid_loss).\n",
        "\n",
        "Finally, the function returns the average training loss (train_loss), average training accuracy (train_acc), average validation loss (valid_loss) and average validation accuracy (valid_acc) as a tuple. These values can be used to monitor and evaluate the performance of the neural network during training."
      ],
      "metadata": {
        "id": "9LKsV2iy-P8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, train_loader, val_loader, optimizer, cost_function, device='cuda'):\n",
        "\n",
        "    # set the network to training mode\n",
        "    net.train()\n",
        "\n",
        "    # train loop\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    # iterate over training data\n",
        "    for batch in train_loader:\n",
        "\n",
        "        # move inputs and targets to specified device\n",
        "        inputs = batch['mask'].float().to(device)\n",
        "        targets = batch['label'].to(device)\n",
        "        # repeat the grayscale channel 3 times to create a 3-channel image\n",
        "        inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # zero the optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # compute the loss\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute training accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        num_train_samples += targets.size(0)\n",
        "        train_acc += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # track training loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # compute average training loss and accuracy\n",
        "    train_loss /= num_train_samples\n",
        "    train_acc /= num_train_samples\n",
        "\n",
        "    # set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # validation loop\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    num_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch['mask'].float().to(device)\n",
        "            targets = batch['label'].to(device)\n",
        "            inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            num_val_samples += targets.size(0)\n",
        "            val_acc += predicted.eq(targets).sum().item()\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= num_val_samples\n",
        "    val_acc /= num_val_samples\n",
        "\n",
        "    return train_loss, train_acc, val_loss, val_acc"
      ],
      "metadata": {
        "id": "AxIuP71pjpgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell in question defines a Python function called test_step, which is used to test a neural network during the validation or testing phase.\n",
        "\n",
        "The function takes as input the neural network (net), a data loader (data_loader), a cost function (cost_function) and a device (device) on which to run the test.\n",
        "\n",
        "At the beginning of the function, the neural network is set to evaluation mode (net.eval()). Next, a for loop is created to run through the data loader, which contains the test data set. For each batch of inputs and targets in the data loader, the function performs the following steps:\n",
        "\n",
        "Moving the inputs and targets to the specified device (device). Forward pass of the neural network (outputs = net(inputs)). Calculating the cost function (loss = cost_function(outputs, targets)). Update of evaluation metrics, specifically: Calculating total accuracy (total_acc) by adding the total number of samples in the batch (targets.size(0)) to the variable total_acc. Calculation of correct accuracy (correct_acc) by adding the number of correct predictions ((predicted == targets).sum().item()) to the correct_acc variable. Calculating total loss (loss_acc) by summing the batch loss (loss.item()) multiplied by the total number of batch samples (targets.size(0)) to the variable loss_acc. At the end of the function, the function returns the average test loss (loss_acc / total_acc) and average test accuracy (correct_acc / total_acc) as a tuple. These values can be used to evaluate the performance of the neural network on the test data."
      ],
      "metadata": {
        "id": "cOyL7Wpu-SYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(net, data_loader, cost_function, device):\n",
        "  net.eval()\n",
        "\n",
        "  test_loss = 0.0\n",
        "  test_acc = 0.0\n",
        "  num_test_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch in data_loader:\n",
        "          inputs = batch['mask'].float().to(device)\n",
        "          targets = batch['label'].to(device)\n",
        "          inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "          outputs = net(inputs)\n",
        "          loss = cost_function(outputs, targets)\n",
        "\n",
        "          _, predicted = outputs.max(1)\n",
        "          num_test_samples += targets.size(0)\n",
        "          test_acc += predicted.eq(targets).sum().item()\n",
        "\n",
        "          test_loss += loss.item()\n",
        "\n",
        "  test_loss /= num_test_samples\n",
        "  test_acc /= num_test_samples\n",
        "\n",
        "  return test_loss, test_acc #loss_acc / total_acc, correct_acc / total_acc"
      ],
      "metadata": {
        "id": "aYjUosrljpWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code is a Python function named \"main\" that implements a complete iteration of a neural network training process for classification. The function takes as input the training configuration parameters such as the number of epochs, batch size, learning rate, weight decay, momentum, training data loader, validation data loader, test data loader, optimizer and cost function.\n",
        "\n",
        "Within the function, an Early Stopping criterion is created, which is used to stop training when the validation loss does not improve for a certain number of epochs. Next, the function iterates over the specified number of epochs and at each iteration calls the training_step function to train the network based on the training and validation data loader.\n",
        "\n",
        "During training, the model parameters with the lowest validation loss up to that point are saved. At the end of training, the function uses the saved model parameters to perform the final test on the test data.\n",
        "\n",
        "Finally, the function returns the training, validation and test losses and accuracies as output. The code also provides the ability to save the trained model to disk for later use.\n",
        "\n"
      ],
      "metadata": {
        "id": "_---bmva-Ute"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def main(batch_size,\n",
        "         device,\n",
        "         learning_rate,\n",
        "         weight_decay,\n",
        "         momentum,\n",
        "         epochs,\n",
        "         train_loader,\n",
        "         val_loader,\n",
        "         test_loader,\n",
        "         optimizer,\n",
        "         cost_function):\n",
        "\n",
        "    # Create the early stopping criterion\n",
        "    #criterion = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "    # initialize variables to keep track of best validation loss and corresponding model parameters\n",
        "    best_loss = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    # iterate over the number of epochs\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # train & log\n",
        "        train_loss, train_acc, val_loss, val_acc = training_step(model_ft, train_loader, val_loader, optimizer, cost_function, device)\n",
        "\n",
        "        # check if the validation loss has improved\n",
        "        if val_loss < best_loss :\n",
        "            best_loss = val_loss\n",
        "            best_params = copy.deepcopy(model_ft.state_dict())\n",
        "\n",
        "        # log epoch results\n",
        "        print(f'Epoch {e+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Val Loss: {val_loss:.4f}, , Val Accuracy: {val_acc:.4f}')\n",
        "\n",
        "    # load the best model parameters\n",
        "    model_ft.load_state_dict(best_params)\n",
        "\n",
        "    # test the model on the test set\n",
        "    test_loss, test_acc = test_step(model_ft, test_loader, cost_function, device)\n",
        "\n",
        "    # log test results\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    return train_loss, train_acc, val_loss, val_acc, test_loss, test_acc"
      ],
      "metadata": {
        "id": "ehTO4ei3jpK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ltrain_loss = []\n",
        "train_loss, train_acc, val_loss, val_acc, test_loss, test_acc = main(batch_size, device, learning_rate, weight_decay, momentum, epoch, dataloader_train_classification, dataloader_val_classification, dataloader_test_classification, optimizer, cost_function)"
      ],
      "metadata": {
        "id": "x0tYjedNiEIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_C7TjaBC4jFV",
        "tso1VGl74ou1",
        "6lpnfTv_woLt",
        "0ppOiB9btjFb",
        "1qwsKZxG6G3l",
        "e_5N7X4N83ib",
        "Yyc6WhUSGnwY",
        "nLbKOxeV43FM"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
